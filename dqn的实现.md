![[Pasted image 20260213125132.png]]
DQN = Q-Learning + æ·±åº¦ç¥ç»ç½‘ç»œ (Deep Learning)ã€‚
è¿™å¼ å›¾ç‰‡å±•ç¤ºçš„æ˜¯å¼ºåŒ–å­¦ä¹ é¢†åŸŸéå¸¸ç»å…¸çš„ä¸€ä¸ªç®—æ³•ï¼š**å¸¦æœ‰ç»éªŒå›æ”¾çš„æ·±åº¦ Q ç½‘ç»œï¼ˆDeep Q-learning with experience replayï¼Œé€šå¸¸ç®€ç§°ä¸º DQNï¼‰** çš„ä¼ªä»£ç ã€‚

è¿™ä¸ªç®—æ³•ç”± DeepMind æå‡ºï¼Œæ ‡å¿—ç€æ·±åº¦å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆçš„é‡å¤§çªç ´ï¼ˆæœ€åˆç”¨äºè®© AI ç©é›…è¾¾åˆ©æ¸¸æˆï¼‰ã€‚å®ƒè§£å†³äº†ä½¿ç”¨ç¥ç»ç½‘ç»œé€¼è¿‘ Q å‡½æ•°æ—¶å®¹æ˜“å‡ºç°çš„ä¸ç¨³å®šå’Œå‘æ•£é—®é¢˜ã€‚

ä¸ºäº†è®©ä½ æ›´æ¸…æ™°åœ°ç†è§£ï¼Œæˆ‘ä»¬æŠŠè¿™ç«¯ä¼ªä»£ç æ‹†è§£æˆå‡ ä¸ªæ ¸å¿ƒéƒ¨åˆ†æ¥çœ‹çœ‹ï¼š

### 1. åˆå§‹åŒ–é˜¶æ®µ (Initialization)

åœ¨æ­£å¼å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œç®—æ³•éœ€è¦å‡†å¤‡ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼š

- **Initialize replay memory $D$ to capacity $N$**: åˆå§‹åŒ–ä¸€ä¸ªå®¹é‡ä¸º $N$ çš„â€œç»éªŒå›æ”¾æ± â€ï¼ˆReplay Bufferï¼‰ã€‚å®ƒå°±åƒæ˜¯æ™ºèƒ½ä½“çš„è®°å¿†åº“ï¼Œç”¨æ¥å­˜æ”¾è¿‡å»çš„ç»å†ã€‚
    
- **Initialize action-value function $Q$ with random weights $\theta$**: åˆå§‹åŒ–**ä¸» Q ç½‘ç»œ**ï¼ˆMain Networkï¼‰ï¼Œå‚æ•°ä¸º $\theta$ã€‚è¿™ä¸ªç½‘ç»œè´Ÿè´£åœ¨æ¯ä¸€æ­¥é€‰æ‹©åŠ¨ä½œï¼Œå¹¶åœ¨è®­ç»ƒä¸­ä¸æ–­æ›´æ–°ã€‚
    
- **Initialize target action-value function $\hat{Q}$ with weights $\theta^- = \theta$**: åˆå§‹åŒ–**ç›®æ ‡ Q ç½‘ç»œ**ï¼ˆTarget Networkï¼‰ï¼Œåˆå§‹å‚æ•° $\theta^-$ å’Œä¸»ç½‘ç»œå®Œå…¨ä¸€æ ·ã€‚è¿™æ˜¯ DQN ä¿æŒç¨³å®šçš„ä¸€ä¸ªå…³é”®æŠ€å·§ï¼ˆåŒç½‘ç»œç»“æ„ï¼‰ã€‚
    

### 2. äº¤äº’ä¸æ•°æ®æ”¶é›† (Interaction & Data Collection)

ä»£ç è¿›å…¥äº†ä¸¤ä¸ªå¾ªç¯ï¼šå¤–å±‚æ˜¯ Episodeï¼ˆå›åˆï¼‰ï¼Œå†…å±‚æ˜¯ Time stepï¼ˆæ—¶é—´æ­¥ï¼‰ã€‚

- **$\epsilon$-greedy ç­–ç•¥é€‰æ‹©åŠ¨ä½œ**: åœ¨æ¯ä¸ªæ—¶é—´æ­¥ $t$ï¼Œæ™ºèƒ½ä½“ä»¥ $\epsilon$ çš„æ¦‚ç‡éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼ˆæ¢ç´¢ç¯å¢ƒï¼‰ï¼Œå¦åˆ™é€‰æ‹©å½“å‰ Q ç½‘ç»œè®¤ä¸ºä»·å€¼æœ€å¤§çš„åŠ¨ä½œ $\operatorname{argmax}_a Q(\phi(s_t), a; \theta)$ï¼ˆåˆ©ç”¨ç°æœ‰çŸ¥è¯†ï¼‰ã€‚
    
- **æ‰§è¡ŒåŠ¨ä½œä¸è§‚å¯Ÿ**: æ™ºèƒ½ä½“åœ¨ç¯å¢ƒï¼ˆä»¿çœŸå™¨ï¼‰ä¸­æ‰§è¡ŒåŠ¨ä½œ $a_t$ï¼Œå¾—åˆ°å¥–åŠ± $r_t$ å’Œä¸‹ä¸€ä¸ªç”»é¢çš„è§‚æµ‹å€¼ $x_{t+1}$ã€‚
    
- **çŠ¶æ€å¤„ç†**: å°†ä¿¡æ¯ç»„åˆå¹¶é¢„å¤„ç†æˆçŠ¶æ€ $\phi_{t+1}$ï¼ˆåœ¨åŸå§‹è®ºæ–‡ä¸­ï¼Œé€šå¸¸æ˜¯æŠŠè¿ç»­å‡ å¸§å›¾åƒå åœ¨ä¸€èµ·ä½œä¸ºè¾“å…¥ï¼Œä»¥æ•æ‰åŠ¨æ€ä¿¡æ¯ï¼‰ã€‚
    
- **å­˜å‚¨ç»éªŒ (Store transition)**: å°†è¿™ä¸€æ­¥çš„å®Œæ•´ç»å† $(\phi_t, a_t, r_t, \phi_{t+1})$ æ‰“åŒ…æˆä¸€ä¸ªå…ƒç»„ï¼Œå­˜å…¥åˆšæ‰åˆå§‹åŒ–çš„ç»éªŒå›æ”¾æ±  $D$ ä¸­ã€‚
    

### 3. å­¦ä¹ ä¸ç½‘ç»œæ›´æ–° (Learning & Optimization)

è¿™æ˜¯ç®—æ³•æœ€æ ¸å¿ƒçš„è®­ç»ƒè¿‡ç¨‹ï¼Œå‘ç”Ÿåœ¨æ¯ä¸€æ­¥æ•°æ®å­˜å…¥å›æ”¾æ± ä¹‹åï¼š

- **ç»éªŒå›æ”¾ (Sample random minibatch)**: ä»å›æ”¾æ±  $D$ ä¸­**éšæœº**æŠ½å–ä¸€å°æ‰¹ï¼ˆminibatchï¼‰å†å²ç»éªŒã€‚_è¿™ä¸€æ­¥æ‰“ç ´äº†è¿ç»­æ•°æ®ä¹‹é—´çš„æ—¶é—´ç›¸å…³æ€§ï¼Œè®©ç¥ç»ç½‘ç»œè®­ç»ƒæ›´åŠ ç¨³å®šã€‚_
    
- **è®¡ç®—ç›®æ ‡å€¼ (Set $y_j$)**: è¿™é‡Œè®¡ç®—çš„æ˜¯ TD Targetï¼ˆæ—¶åºå·®åˆ†ç›®æ ‡ï¼‰ï¼Œå®ƒæ˜¯ç¥ç»ç½‘ç»œè¯•å›¾é€¼è¿‘çš„â€œæ­£ç¡®ç­”æ¡ˆâ€ï¼š
    
    - å¦‚æœè¿™ä¸€æ­¥æ¸¸æˆç»“æŸäº†ï¼ˆepisode terminatesï¼‰ï¼Œç›®æ ‡å€¼å°±æ˜¯å½“å‰çš„å¥–åŠ± $r_j$ã€‚
        
    - å¦‚æœæ¸¸æˆè¿˜æ²¡ç»“æŸï¼Œç›®æ ‡å€¼å°±æ˜¯ï¼š$r_j + \gamma \max_{a'} \hat{Q}(\phi_{j+1}, a'; \theta^-)$ã€‚**æ³¨æ„è¿™é‡Œä½¿ç”¨çš„æ˜¯ç›®æ ‡ç½‘ç»œ $\hat{Q}$**ï¼Œè€Œä¸æ˜¯ä¸»ç½‘ç»œã€‚å®ƒè¯„ä¼°çš„æ˜¯åœ¨ä¸‹ä¸€ä¸ªçŠ¶æ€ä¸‹èƒ½æ‹¿åˆ°çš„æœ€å¤§æœªæ¥é¢„æœŸå¥–åŠ±ï¼ˆ$\gamma$ æ˜¯æŠ˜æ‰£å› å­ï¼‰ã€‚
        
- **æ¢¯åº¦ä¸‹é™ (Gradient descent)**: è®¡ç®—ç›®æ ‡å€¼ $y_j$ ä¸ä¸»ç½‘ç»œå½“å‰é¢„æµ‹å€¼ $Q(\phi_j, a_j; \theta)$ ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSE Lossï¼‰ï¼Œç„¶åé€šè¿‡æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°**ä¸»ç½‘ç»œ**çš„å‚æ•° $\theta$ã€‚
    
- **æ›´æ–°ç›®æ ‡ç½‘ç»œ (Reset $\hat{Q} = Q$)**: æ¯ç»è¿‡ $C$ ä¸ªæ­¥é•¿ï¼Œå°±æŠŠä¸»ç½‘ç»œçš„å‚æ•° $\theta$ å¤åˆ¶ç»™ç›®æ ‡ç½‘ç»œçš„å‚æ•° $\theta^-$ã€‚è¿™ç§â€œå»¶è¿Ÿæ›´æ–°â€é¿å…äº†ç›®æ ‡å€¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‰§çƒˆæ³¢åŠ¨ï¼Œç›¸å½“äºåœ¨è¿½é€ä¸€ä¸ªç›¸å¯¹é™æ­¢çš„é¶å­ã€‚
    

---

**æ€»ç»“æ¥è¯´ï¼Œè¿™ä¸ªç®—æ³•çš„ä¸¤å¤§çµé­‚æœºåˆ¶æ˜¯ï¼š**

1. **Experience Replayï¼ˆç»éªŒå›æ”¾ï¼‰**ï¼šéšæœºæ‰“ä¹±è¿‡å»çš„æ•°æ®æ¥è®­ç»ƒï¼Œæé«˜æ•°æ®åˆ©ç”¨ç‡å¹¶ç¨³å®šè®­ç»ƒã€‚
    
2. **Target Networkï¼ˆç›®æ ‡ç½‘ç»œï¼‰**ï¼šç”¨ä¸€ä¸ªæ›´æ–°é¢‘ç‡è¾ƒæ…¢çš„ç‹¬ç«‹ç½‘ç»œæ¥è®¡ç®— Loss çš„ç›®æ ‡å€¼ï¼Œé˜²æ­¢è®­ç»ƒè¿‡ç¨‹å‡ºç°éœ‡è¡ï¼ˆå³è‡ªå·±è¿½ç€è‡ªå·±çš„å°¾å·´è·‘ï¼‰ã€‚

### 3.ä»£ç å®ç°
ç”±äºæƒ³è¦å¿«é€ŸéªŒè¯è¿™ä¸ªdqnçš„å®ç°ï¼Œæˆ‘é€‰æ‹©äº†ä½¿ç”¨flybirdä½œä¸ºè®­ç»ƒçš„å¯¹è±¡ï¼Œä»¥ä¸‹æ˜¯flybirdçš„æ¸¸æˆç®€ä»‹ï¼š
###### æ¸¸æˆä»‹ç»

ã€ŠFlappy Birdã€‹ï¼ˆåƒç´ é¸Ÿï¼‰æ˜¯ä¸€æ¬¾é‡‡ç”¨å¤å¤åƒç´ ç”»é£çš„æ— å°½ç”Ÿå­˜ç±»è¡—æœºæ¸¸æˆï¼Œå…¶**ç©æ³•**æå…¶æç®€ï¼Œç©å®¶åªéœ€é€šè¿‡ä¸æ–­ç‚¹å‡»æŒ‰é”®æ¥æ§åˆ¶å°é¸ŸæŒ¯ç¿…å¯¹æŠ—é‡åŠ›ï¼Œä½¿å…¶åœ¨ä¸æ–­å‘å·¦ç§»åŠ¨ã€é«˜ä½ä¸ä¸€çš„ç»¿è‰²æ°´ç®¡ç¼éš™ä¸­ç²¾å‡†ç©¿æ¢­ï¼›æ¸¸æˆçš„**å¾—åˆ†æœºåˆ¶**ç®€å•ç›´ç™½ï¼Œå°é¸Ÿæ¯æˆåŠŸå®‰å…¨ç©¿è¿‡ä¸€å¯¹æ°´ç®¡å³å¯è·å¾— 1 åˆ†ï¼Œå¹¶ä¸”æ²¡æœ‰æœ€ç»ˆçš„é€šå…³ç»ˆç‚¹ï¼›è€Œè¯¥æ¸¸æˆçš„**æ ¸å¿ƒç‰¹æ€§**åˆ™åœ¨äºå…¶ç¡¬æ ¸çš„â€œé›¶å®¹é”™ç‡â€ä¸â€œé«˜é¢‘è¯•é”™â€æœºåˆ¶â€”â€”å°é¸Ÿä¸€æ—¦è§¦ç¢°æ°´ç®¡æˆ–æ‰è½åœ°é¢å°±ä¼šç¬é—´æ­»äº¡å¹¶ç«‹åˆ»é‡ç½®ï¼Œè¿™ç§æ“ä½œæç®€ä½†æéš¾ç²¾é€šçš„è®¾å®šï¼Œé…åˆç€æ— å°½æ¨¡å¼çš„å¿ƒç†å‹åŠ›ï¼Œé€ å°±äº†å®ƒæé«˜çš„æŒ‘æˆ˜æ€§å’Œä»¤äººæåº¦ä¸Šç˜¾çš„å±æ€§ã€‚

#### ä»£ç æµç¨‹å›¾
ç®—æ³•ä¸€èˆ¬æµç¨‹å›¾
![[Pasted image 20260213202610.png]]
![[Pasted image 20260213202622.png]]
![[Pasted image 20260213202632.png]]
![[Pasted image 20260213202642.png]]
æœ¬é¡¹ç›®æµç¨‹å›¾
![[Radar PSLR_ISLR TBP Flow-2026-02-13-083214.svg]]
#### ç¯å¢ƒé€‰æ‹©
æœ¬å®éªŒé‡‡ç”¨ Gymnasium (åŸ OpenAI Gym) ä½œä¸ºå¼ºåŒ–å­¦ä¹ çš„ç¯å¢ƒæ¡†æ¶ã€‚é€‰æ‹©è¯¥æ¡†æ¶æ˜¯å› ä¸ºå®ƒæä¾›äº†ä¸€å¥—é«˜åº¦æ ‡å‡†åŒ–çš„ç¯å¢ƒäº¤äº’ API (`step`, `reset`)ï¼Œå®ç°äº†æ™ºèƒ½ä½“ç®—æ³•ä¸åº•å±‚ç¯å¢ƒç‰©ç†é€»è¾‘çš„å®Œç¾è§£è€¦ã€‚è¿™ä¸ä»…æå¤§åœ°æé«˜äº†ç®—æ³•ä»£ç çš„å¤ç”¨æ€§ï¼Œä¹Ÿä½¿å¾—æœ¬å®éªŒçš„ç»“æœå…·å¤‡äº†ä¸å›½é™…ä¸»æµå¼ºåŒ–å­¦ä¹ åŸºå‡†ï¼ˆBenchmarkï¼‰è¿›è¡Œå…¬å¹³å¯¹æ¯”çš„å­¦æœ¯ä»·å€¼ã€‚

---
æœ€å¸å¼•æˆ‘çš„ä¸€ç‚¹å°±æ˜¯ï¼Œæˆ‘ä¸ç”¨åœ¨è€—è´¹æ—¶é—´åœ¨ç¯å¢ƒçš„æ­å»ºä¸Šï¼Œåªéœ€è¦æ¥æ³¨æ„agentçš„è®¾è®¡ä¸ç®—æ³•çš„è®¾è®¡å°±å¯ä»¥äº†ï¼Œå¹¶ä¸”å‡å¦‚æ˜å¤©ä½ çš„å¯¼å¸ˆè¯´ï¼šâ€œåˆ«æåƒç´ é¸Ÿäº†ï¼Œå»æä¸ªå€’ç«‹æ‘†ï¼ˆCartPoleï¼‰æˆ–è€…èµ°è¿·å®«å§ï¼â€ ä½ **å®Œå…¨ä¸éœ€è¦ä¿®æ”¹ `dqn_agent.py` é‡Œçš„ä»»ä½•ä¸€è¡Œæ ¸å¿ƒç®—æ³•ä»£ç **ï¼Œä½ åªéœ€è¦åœ¨ `train.py` é‡ŒæŠŠä¸€è¡Œä»£ç æ”¹æ‰ï¼š

- ä»ï¼š`env = gym.make("FlappyBird-v0")`
    
- æ”¹æˆï¼š`env = gym.make("CartPole-v1")`
ä½ çš„ AI å°±èƒ½ç«‹åˆ»æ— ç¼è¡”æ¥ï¼Œå¼€å§‹å­¦ä¹ æ–°æ¸¸æˆã€‚è¿™å°±å«â€œä¸€æ¬¡ç¼–å†™ï¼Œåˆ°å¤„è¿è¡Œâ€ã€‚

#### ä»£ç ç†è§£
##### ä¸²è¡Œä»£ç ç†è§£
---
    å› ä¸ºæ˜¯ç¬¬ä¸€æ¬¡å†™è¿™æ ·çš„ä»£ç ï¼Œæ‰€ä»¥æ˜¯å…ˆè®©aiå¸®æˆ‘å†™å‡ºæ¥ç„¶åæˆ‘è‡ªå·±æ ¹æ®ç†è§£æ¥è¿›è¡Œä¿®æ”¹ï¼Œä¹Ÿæ˜¯å› ä¸ºæ˜¯ç¬¬ä¸€æ¬¡å†™è¿™ä¸ªä»£ç æ‰€ä»¥å¯èƒ½ä¼šæ³¨é‡Šå¾ˆå¤šçš„åŸºç¡€çŸ¥è¯†ã€‚

---
é¦–å…ˆï¼Œæ–‡ä»¶æ˜¯ç”±ä¸‰éƒ¨åˆ†æ„æˆï¼Œtrain.py,dqn_agent.py,test.pyï¼Œåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¹‹ä¸­æˆ‘ä»¬ä¸ä½¿ç”¨æ¸²æŸ“çš„æ–¹æ³•ï¼Œå¯ä»¥åŠ é€Ÿæˆ‘ä»¬çš„è®­ç»ƒçš„è¿‡ç¨‹ï¼Œç„¶åå†test.pyé‡Œé¢å°±æ˜¯æŠŠæˆ‘ä»¬çš„è®­ç»ƒçš„ç»“æœæ‹¿è¿‡æ¥ä½¿ç”¨ï¼Œè¿™æ—¶å€™ï¼Œä¸ºäº†æ›´åŠ ç›´è§‚åœ°å±•ç¤ºè¿™ä¸ªåŠ¨ç”»ï¼Œæˆ‘ä»¬å°±ä¼šæŠŠä»–æ¸²æŸ“å‡ºæ¥ã€‚dqn_agen.pyå°±æ˜¯æˆ‘ä»¬çš„å¤§è„‘è´Ÿè´£å†³ç­–çš„ä»£ç†ã€‚

```
env = gymnasium.make("FlappyBird-v0", use_lidar=False)
```
 `use_lidar=False`ï¼ˆè¿™ä¸ªåº“ç‰¹æœ‰çš„å‚æ•°ï¼‰

- **å«ä¹‰**ï¼š`lidar` æ˜¯â€œæ¿€å…‰é›·è¾¾â€çš„æ„æ€ã€‚`False` è¡¨ç¤ºå…³é—­å®ƒã€‚
    
- **ä½œç”¨**ï¼šå¦‚æœè®¾ä¸º `True`ï¼Œç¯å¢ƒä¼šæ¨¡æ‹Ÿå‡ºå‡ æ¡æ¿€å…‰å°„çº¿ä»é¸Ÿçš„èº«ä¸Šå‘å°„å‡ºå»ï¼Œç”¨æ¥æ¢æµ‹ç®¡å­çš„è·ç¦»        ï¼ˆè¿”å›çš„æ•°æ®ä¼šéå¸¸åºå¤§å’Œå¤æ‚ï¼Œé€šå¸¸æ˜¯ä¸€å¤§ä¸²è·ç¦»æ•°å€¼ï¼‰ã€‚
        
    - è®¾ä¸º **`False`** åï¼Œç¯å¢ƒå°±ä¼šå˜å¾—éå¸¸æ¸…çˆ½ï¼Œå®ƒåªä¼šè¿”å›ç»™ä½ æœ€æ ¸å¿ƒçš„ **12 ä¸ªæ•°å­—**ï¼ˆæ¯”å¦‚é¸Ÿçš„é«˜åº¦ã€é€Ÿåº¦ã€ä¸‹ä¸€æ ¹ç®¡å­çš„ X å’Œ Y åæ ‡ç­‰ï¼‰ã€‚
        
    - **ç»“è®º**ï¼šå¯¹äºåˆå­¦è€…å’Œæˆ‘ä»¬å†™çš„ç®€æ˜“ DQN æ¥è¯´ï¼Œå…³é—­é›·è¾¾ï¼ˆ`use_lidar=False`ï¼‰èƒ½è®© AI æ›´å®¹æ˜“çœ‹æ‡‚æ•°æ®ï¼Œè®­ç»ƒé€Ÿåº¦ä¼šå¿«å¾—å¤šã€‚
 `FlappyBird-v0"`

- **å«ä¹‰**ï¼šç¯å¢ƒçš„å”¯ä¸€è¯†åˆ«ç ï¼ˆIDï¼‰ã€‚
    
- **ä½œç”¨**ï¼šå‘Šè¯‰å·¥å‚ä½ è¦ç©å“ªæ¬¾æ¸¸æˆã€‚`FlappyBird` æ˜¯æ¸¸æˆåï¼Œ`-v0` ä»£è¡¨è¿™æ˜¯è¯¥æ¸¸æˆçš„ç¬¬ 0 ä¸ªç‰ˆæœ¬ï¼ˆå¦‚æœä»¥åä½œè€…ä¿®å¤äº† bug æˆ–æ”¹äº†è§„åˆ™ï¼Œå¯èƒ½ä¼šæ¨å‡º `-v1`ã€`-v2`ï¼‰ã€‚

```
state_dim = env.observation_space.shape[0] # 12

Â  Â  action_dim = env.action_space.n Â  Â  Â  Â  Â  Â # 2
```
 1. `state_dim = env.observation_space.shape[0]` (çŠ¶æ€ç»´åº¦)

- **å­—é¢æ„æ€**ï¼šè·å–ç¯å¢ƒï¼ˆenvï¼‰è§‚æµ‹ç©ºé—´ï¼ˆobservation_spaceï¼‰çš„ç¬¬ä¸€ä¸ªå½¢çŠ¶æ•°å€¼ï¼ˆshape[0]ï¼‰ã€‚
    
- **é€šä¿—ç†è§£**ï¼š**â€œAI çš„è¾“å…¥æœ‰å¤šå°‘ä¸ªæ•°å­—ï¼Ÿâ€**
    
- **é’ˆå¯¹ Flappy Bird**ï¼šè¿™ä¸ªåº“è§„å®šäº†å®ƒä¼šè¿”å› 12 ä¸ªå…³é”®æ•°å­—ç»™ AIï¼ˆåŒ…æ‹¬é¸Ÿçš„é«˜åº¦ã€é€Ÿåº¦ã€ç¦»ä¸‹ä¸€æ ¹ç®¡å­çš„æ°´å¹³è·ç¦»ã€ç®¡å­ä¸Šä¸‹æ²¿çš„é«˜åº¦ç­‰ï¼‰ã€‚
    
    - æ‰€ä»¥è¿™é‡Œçš„ `state_dim` ç»“æœå°±æ˜¯ **12**ã€‚
        
- **ä¸ºä»€ä¹ˆè¿™ä¹ˆå†™**ï¼šä¸ºäº†**é€šç”¨æ€§**ã€‚å¦‚æœä½ æ˜å¤©æŠŠç¯å¢ƒæ¢æˆäº†â€œå€’ç«‹æ‘†â€ï¼Œé‚£é‡Œçš„çŠ¶æ€åªæœ‰ 4 ä¸ªæ•°å­—ï¼Œä»£ç ä¼šè‡ªåŠ¨æŠŠ `state_dim` è®¾ä¸º 4ï¼Œè€Œä¸éœ€è¦ä½ æ‰‹åŠ¨å»æ”¹ã€‚
    

---

 2. `action_dim = env.action_space.n` (åŠ¨ä½œç»´åº¦)

- **å­—é¢æ„æ€**ï¼šè·å–ç¯å¢ƒï¼ˆenvï¼‰åŠ¨ä½œç©ºé—´ï¼ˆaction_spaceï¼‰çš„æ•°é‡ï¼ˆnï¼‰ã€‚
    
- **é€šä¿—ç†è§£**ï¼š**â€œAI æœ‰å¤šå°‘ç§é€‰æ‹©ï¼Ÿâ€**
    
- **é’ˆå¯¹ Flappy Bird**ï¼šè¿™ä¸ªæ¸¸æˆé‡Œä½ åªæœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š
    
    1. `0`ï¼šä»€ä¹ˆéƒ½ä¸åšï¼ˆä»»ç”±é‡åŠ›ä¸‹å ï¼‰ã€‚
        
    2. `1`ï¼šç‚¹å‡»å±å¹•ï¼ˆå‘ä¸Šæ‰‡åŠ¨ç¿…è†€ï¼‰ã€‚
        
    
    - æ‰€ä»¥è¿™é‡Œçš„ `action_dim` ç»“æœå°±æ˜¯ **2**ã€‚
        
- **ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆå†™**ï¼šåŒæ ·æ˜¯ä¸ºäº†**é€šç”¨æ€§**ã€‚å¦‚æœæ¢æˆä¸€ä¸ªæœ‰â€œä¸Šä¸‹å·¦å³â€å››ä¸ªé”®çš„æ¸¸æˆï¼Œ`action_dim` å°±ä¼šè‡ªåŠ¨å˜æˆ 4ã€‚

```
Â # è®­ç»ƒè¶…å‚æ•°

Â  Â  EPISODES = 100000

Â  Â  BATCH_SIZE = 64

Â  Â  TARGET_UPDATE_FREQ = 10 # æ¯è¿‡ 10 å±€åŒæ­¥ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
```

EPISODESä»£è¡¨çš„æ˜¯è®­ç»ƒçš„æ¬¡æ•°ä½ å¯ä»¥ç†è§£ä¸ºæœ‰å¤šå°‘æ¡å‘½

`BATCH_SIZE = 64`ï¼ˆæ‰¹é‡å¤§å°ï¼‰

- **ç‰©ç†æ„ä¹‰**ï¼šæ¯æ¬¡â€œå¤§è„‘æ›´æ–°â€æ—¶ï¼Œä»è®°å¿†åº“ä¸­æŠ½å–çš„æ ·æœ¬æ•°é‡ã€‚
    
- **ä¸²è¡Œé€»è¾‘**ï¼š
    
    1. å°é¸Ÿæ¯é£ä¸€æ­¥ï¼Œå°±æŠŠ `(å½“å‰çŠ¶æ€, åŠ¨ä½œ, å¥–åŠ±, ä¸‹ä¸ªçŠ¶æ€)` å­˜è¿›ä¹¦åŒ…ï¼ˆç»éªŒå›æ”¾æ± ï¼‰ã€‚
        
    2. å½“ä¹¦åŒ…é‡Œçš„æ•°æ®å¤Ÿå¤šæ—¶ï¼ŒAI ä¼šåœä¸‹æ¥ï¼Œéšæœºä»ä¹¦åŒ…é‡ŒæŠ“å‡º **64 æ¡** å†å²è®°å½•ã€‚
        
    3. ç¥ç»ç½‘ç»œæ ¹æ®è¿™ 64 æ¡è®°å½•ç®—å‡ºä¸€ä¸ªå¹³å‡çš„è¯¯å·®ï¼Œç„¶åè°ƒæ•´è‡ªå·±çš„å‚æ•°ã€‚
        
- **ä¸ºä»€ä¹ˆè¦ 64ï¼Ÿ**ï¼šå¦‚æœä½ åªæ‹¿ 1 æ¡è®°å½•æ›´æ–°ï¼ŒAI å®¹æ˜“è¢«æŸä¸€æ¬¡å¶ç„¶çš„å¹¸è¿æˆ–å€’éœ‰å¸¦åï¼ˆéšæœºæ€§å¤ªå¤§ï¼‰ï¼›æ‹¿ 64 æ¡ä¸€èµ·çœ‹ï¼Œå®ƒèƒ½å­¦åˆ°æ›´ç¨³å¥çš„è§„å¾‹ã€‚
- 
 `TARGET_UPDATE_FREQ = 10`ï¼ˆç›®æ ‡ç½‘ç»œåŒæ­¥é¢‘ç‡ï¼‰

- **è¿™æ˜¯æœ€å…³é”®çš„ç¨³å®šæ€§è®¾è®¡ï¼**
    
- **èƒŒæ™¯çŸ¥è¯†**ï¼šDQN å†…éƒ¨æœ‰ä¸¤ä¸ªä¸€æ¨¡ä¸€æ ·çš„ç½‘ç»œï¼š
    
    1. **Policy Net (ç­–ç•¥ç½‘ç»œ)**ï¼šå®ƒæ˜¯â€œå­¦ç”Ÿâ€ï¼Œæ—¶åˆ»åœ¨å­¦ä¹ ï¼Œå‚æ•°ä¸€ç›´åœ¨å˜ã€‚
        
    2. **Target Net (ç›®æ ‡ç½‘ç»œ)**ï¼šå®ƒæ˜¯â€œå‚è€ƒä¹¦â€ï¼Œç”¨æ¥è®¡ç®—ç›®æ ‡åˆ†æ•°å€¼ã€‚
```
state, info = env.reset()

Â  Â  Â  Â  total_reward = 0

Â  Â  Â  Â  score = 0 # è®°å½•ç©¿è¿‡çš„ç®¡å­æ•°

Â  Â  Â  Â  done = False
```

 `state, info = env.reset()`

- **åŠ¨ä½œ**ï¼šç¯å¢ƒå½»åº•é‡å¯ã€‚
    
- **ç‰©ç†æ„ä¹‰**ï¼šå°é¸Ÿå›åˆ°å‡ºå‘ç‚¹ï¼ˆé€šå¸¸æ˜¯å±å¹•å·¦ä¾§ä¸­é—´ï¼‰ï¼Œæ°´ç®¡é‡æ–°éšæœºç”Ÿæˆã€‚
    
- **è¿”å›å€¼**ï¼š
    
    - `state`: åˆå§‹çš„ **12 ä¸ªæ•°å­—**ï¼ˆåæ ‡ã€é€Ÿåº¦ç­‰ï¼‰ã€‚è¿™æ˜¯ AI çå¼€çœ¼çœ‹åˆ°çš„ç¬¬ä¸€å¹•ã€‚
        
    - `info`: ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä¸€äº›è¾…åŠ©ä¿¡æ¯ï¼ˆæ¯”å¦‚å½“å‰å¾—åˆ†ã€æ¸¸æˆæ¨¡å¼ç­‰ï¼‰ï¼Œé€šå¸¸è®­ç»ƒæ—¶ç”¨ä¸åˆ°ï¼Œä½†å¿…é¡»æ¥æ”¶å®ƒã€‚

 `done = False`

- **ç‰©ç†æ„ä¹‰**ï¼šè®¾ç½®ä¸€ä¸ªâ€œæ­»äº¡æ ‡å¿—ä½â€ã€‚
    
- **é€»è¾‘ä½œç”¨**ï¼šåªè¦å°é¸Ÿæ²¡æ’å¢™ï¼Œ`done` å°±ä¸€ç›´æ˜¯ `False`ã€‚ä¸€æ—¦æ’å¢™ï¼Œç¯å¢ƒä¼šè¿”å› `done = True`ï¼Œä»è€Œè·³å‡ºå½“å‰çš„ `while` å¾ªç¯ï¼Œç»“æŸè¿™ä¸€å±€ã€‚


```
while not done:

Â  Â  Â  Â  Â  Â  # 1. é€‰åŠ¨ä½œ

Â  Â  Â  Â  Â  Â  action = agent.select_action(state)

Â  Â  Â  Â  Â  Â  # 2. ä¸ç¯å¢ƒäº¤äº’

Â  Â  Â  Â  Â  Â  next_state, reward, terminated, truncated, info = env.step(action)

Â  Â  Â  Â  Â  Â  done = terminated or truncated

Â  Â  Â  Â  Â  Â  # 3. å­˜å…¥ç»éªŒæ± 

Â  Â  Â  Â  Â  Â  agent.memory.push(state, action, reward, next_state, done)

Â  Â  Â  Â  Â  Â  # 4. å­¦ä¹ æ›´æ–°

Â  Â  Â  Â  Â  Â  agent.learn(BATCH_SIZE)

Â  Â  Â  Â  Â  Â  state = next_state

Â  Â  Â  Â  Â  Â  total_reward += reward

Â  Â  Â  Â  Â  Â  score = info.get('score', 0)
```

action = agent.select_action(state)ï¼šAI æŸ¥çœ‹å½“å‰ 12 ä¸ªæ•°å­—ï¼ˆçŠ¶æ€ï¼‰ï¼Œå†³å®šæ˜¯ç‚¹å‡»å±å¹•è¿˜æ˜¯ä»€ä¹ˆéƒ½ä¸åšã€‚

è¿™é‡Œé¢è¿è¡Œç€ **Epsilon-Greedyï¼ˆÎµ-è´ªå©ªï¼‰** ç­–ç•¥ã€‚

- **å‰æœŸ**ï¼šAI å¤§å¤šåœ¨â€œççŒœâ€ï¼ˆæ¢ç´¢ï¼‰ï¼Œå¯»æ‰¾é€šå¾€ç®¡å­çš„è·¯ã€‚
    
- **åæœŸ**ï¼šAI å¤§å¤šåœ¨â€œæŸ¥è¡¨â€ï¼ˆåˆ©ç”¨ï¼‰ï¼Œæ‰§è¡Œå®ƒè®¤ä¸ºå¾—åˆ†æœ€é«˜çš„åŠ¨ä½œã€‚
`terminated/truncated`: æ’å¢™äº†å—ï¼Ÿæˆ–è€…æ¸¸æˆæ˜¯ä¸æ˜¯å¼ºåˆ¶ç»“æŸäº†ï¼Ÿ

##### dqn_agent

```
# è®¾ç½®è®¾å¤‡ (å¦‚æœæœ‰ GPU åˆ™ä½¿ç”¨ GPUï¼Œå¦åˆ™ä½¿ç”¨ CPU)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


```
```
# 1. å®šä¹‰ Q ç¥ç»ç½‘ç»œ (å› ä¸ºè¾“å…¥æ˜¯12ä¸ªæ•°å­—ï¼Œç”¨å…¨è¿æ¥å±‚å³å¯)

class QNetwork(nn.Module):

Â  Â  def __init__(self, state_dim, action_dim):

Â  Â  Â  Â  super(QNetwork, self).__init__()

Â  Â  Â  Â  self.fc1 = nn.Linear(state_dim, 128)

Â  Â  Â  Â  self.fc2 = nn.Linear(128, 128)

Â  Â  Â  Â  self.fc3 = nn.Linear(128, action_dim)

  

Â  Â  def forward(self, x):

Â  Â  Â  Â  x = F.relu(self.fc1(x))

Â  Â  Â  Â  x = F.relu(self.fc2(x))

Â  Â  Â  Â  return self.fc3(x)
```
 self.fc1 = nn.Linear(state_dim, 128)è¾“å…¥å±‚

Â  Â  Â  Â  self.fc2 = nn.Linear(128, 128)éšè—å±‚

Â  Â  Â  Â  self.fc3 = nn.Linear(128, action_dim)è¾“å‡ºå±‚
###### ä¸ºä»€ä¹ˆ Flappy Bird å¸¸ç”¨ 128 æˆ– 256ï¼Ÿ

å¯¹äº 12 ä¸ªè¾“å…¥ç‰¹å¾æ¥è¯´ï¼Œ128 æ˜¯ä¸€ä¸ª**â€œæ€§èƒ½ç”œç‚¹â€**ï¼š

1. **ç‰¹å¾ç»„åˆè¶³å¤Ÿå¤š**ï¼š128 ä¸ªç¥ç»å…ƒè¶³ä»¥ç©·ä¸¾å‡ºâ€œé«˜åº¦â€ã€â€œé€Ÿåº¦â€å’Œâ€œéšœç¢ç‰©è·ç¦»â€ä¹‹é—´çš„å„ç§é€»è¾‘å…³ç³»ã€‚
    
2. **å†—ä½™åº¦é€‚ä¸­**ï¼šå¼ºåŒ–å­¦ä¹ çš„ä¿¡å·ï¼ˆRewardï¼‰éå¸¸å˜ˆæ‚ï¼Œ128 èƒ½å¤Ÿæä¾›ä¸€å®šçš„å®¹é”™ç©ºé—´ï¼Œè®©ç½‘ç»œåœ¨å—åˆ°æŸäº›é”™è¯¯ç»éªŒå†²å‡»æ—¶ï¼Œä¸è‡³äºå…¨ç›˜å´©æºƒã€‚


```
# 2. å®šä¹‰ç»éªŒå›æ”¾æ± 

class ReplayBuffer:

Â  Â  def __init__(self, capacity):

Â  Â  Â  Â  self.buffer = deque(maxlen=capacity)
è¿™æ˜¯ `deque` çš„ç²¾é«“ã€‚å½“å­˜å…¥çš„ç»éªŒè¶…è¿‡å®¹é‡ï¼ˆæ¯”å¦‚ä½ è®¾å®šçš„ 10 ä¸‡æ¡ï¼‰æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨â€œæ’•æ‰â€æœ€æ—§çš„ä¸€é¡µï¼Œå¡«å…¥æ–°çš„ä¸€é¡µã€‚è¿™ä¿è¯äº† AI æ°¸è¿œåœ¨å­¦ä¹ æœ€è¿‘ã€æœ€ç›¸å…³çš„ç»éªŒã€‚
  

Â  Â  def push(self, state, action, reward, next_state, done):

Â  Â  Â  Â  self.buffer.append((state, action, reward, next_state, done))
è®°å½•åˆšæ‰çš„æ•°æ®ï¼Œå®ƒå®Œæ•´è®°å½•äº†**â€œå¤„å¢ƒ(s) -> å†³ç­–(a) -> ç»“æœ(r) -> æ–°å¤„å¢ƒ(s') -> æ­»äº†æ²¡(d)â€**ã€‚è¿™äº”ä¸ªæ•°å­—æ„æˆäº†ä¸€ä¸ªå®Œæ•´çš„å› æœé€»è¾‘é“¾ã€‚
  

Â  Â  def sample(self, batch_size):

Â  Â  Â  Â  batch = random.sample(self.buffer, batch_size)
Â  Â  Â  Â  **`batch`**ï¼šæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé‡Œé¢å¡äº† 64 ä¸ªå°å…ƒç»„ï¼Œåƒè¿™æ ·ï¼š`[(s,a,r,s,d), (s,a,r,s,d), ...]`ã€‚

Â  Â  Â  Â  state, action, reward, next_state, done = map(np.stack, zip(*batch))
Â  Â  Â  Â  æŠŠè¿™äº›åˆ†æ•£çš„æ•°å­—â€œå•ªâ€åœ°ä¸€å£°å‹æˆæ•´é½çš„**çŸ©é˜µï¼ˆTensor/Arrayï¼‰**ã€‚

- **ç»“æœ**ï¼šåŸæ¥çš„ 64 ä¸ªç‹¬ç«‹æ•°å­—ï¼Œå˜æˆäº†å½¢çŠ¶ä¸º `(64, 12)` çš„çŸ©é˜µã€‚è¿™æ ·ä½ çš„ç¥ç»ç½‘ç»œï¼ˆPyTorchï¼‰æ‰èƒ½ä¸€æ¬¡æ€§å¤„ç†å®ƒä»¬ï¼Œå‘æŒ¥æ˜¾å¡çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€‚

Â  Â  Â  Â  return state, action, reward, next_state, done

  

Â  Â  def __len__(self):

Â  Â  Â  Â  return len(self.buffer)
```

```
Â  Â  Â  Â  # è¶…å‚æ•°

Â  Â  Â  Â  self.lr = 1e-3

Â  Â  Â  Â  self.gamma = 0.99

Â  Â  Â  Â  self.epsilon = 1.0 Â  Â  Â  Â  # åˆå§‹æ¢ç´¢ç‡

Â  Â  Â  Â  self.epsilon_min = 0.01 Â  Â # æœ€ä½æ¢ç´¢ç‡

Â  Â  Â  Â  self.epsilon_decay = 0.995 # æ¢ç´¢ç‡è¡°å‡
```
 1. `self.lr = 1e-3` (å­¦ä¹ ç‡ - Learning Rate)

- **ç‰©ç†æ„ä¹‰**ï¼šç¥ç»ç½‘ç»œæƒé‡æ›´æ–°çš„æ­¥é•¿ã€‚
    
- **å½¢è±¡ç†è§£**ï¼š**â€œAI çš„è°¦è™šç¨‹åº¦â€**ã€‚
    
    - å¦‚æœ `lr` å¤ªå¤§ï¼ˆå¦‚ 0.1ï¼‰ï¼šAI å‘ç°ä¸€æ¬¡å¤±è¯¯å°±ä¼šå…¨ç›˜å¦å®šä¹‹å‰çš„ç»éªŒï¼Œå¯¼è‡´è¡¨ç°å‰§çƒˆéœ‡è¡ã€‚
        
    - å¦‚æœ `lr` å¤ªå°ï¼ˆå¦‚ 1e-6ï¼‰ï¼šAI æå…¶å›ºæ‰§ï¼Œå­¦å¾—éå¸¸æ…¢ï¼Œå¯èƒ½è·‘å‡ å¤©å‡ å¤œéƒ½æ²¡ååº”ã€‚
        
- **é’ˆå¯¹ Flappy Bird**ï¼š`1e-3` (å³ 0.001) æ˜¯ä¸€ä¸ªå¾ˆç¨³çš„æ ‡å‡†å€¼ã€‚
    

 2. `self.gamma = 0.99` (æŠ˜æ‰£å› å­ - Discount Factor)

- **ç‰©ç†æ„ä¹‰**ï¼šæœªæ¥å¥–åŠ±çš„ä»·å€¼åœ¨ä»Šå¤©çš„æŠ˜ç°ç‡ã€‚
    
- **å½¢è±¡ç†è§£**ï¼š**â€œAI çš„è¿œè§ç¨‹åº¦â€**ã€‚
    
    - å®ƒè¡¡é‡çš„æ˜¯ï¼š**â€œç°åœ¨çš„å®‰å…¨â€**å’Œ**â€œæœªæ¥çš„å¾—åˆ†â€**å“ªä¸ªæ›´é‡è¦ã€‚
        
    - $\gamma = 0$ï¼šè¿‘è§†çœ¼ã€‚åªç®¡è¿™ä¸€å¸§æ´»æ²¡æ´»ç€ï¼Œå®Œå…¨ä¸ç®¡å‰é¢æœ‰æ²¡æœ‰ç®¡å­ã€‚
        
    - $\gamma = 0.99$ï¼šæ·±è°‹è¿œè™‘ã€‚å®ƒæ„è¯†åˆ°ï¼Œä¸ºäº†é’»è¿‡ 5 ç§’åçš„ç®¡å­ï¼Œç°åœ¨å¿…é¡»æå‰è°ƒæ•´é«˜åº¦ã€‚
        
- **è®¡ç®—å…¬å¼**ï¼šç›®æ ‡ $Q$ å€¼è®¡ç®—çš„æ ¸å¿ƒå…¬å¼æ˜¯ $R + \gamma \times \max Q(s')$ã€‚
    

 3. Epsilon ä¸‰äººç»„ (æ¢ç´¢ä¸åˆ©ç”¨)

è¿™æ˜¯ä½ åˆšæ‰çœ‹åˆ°â€œæœ€é«˜åˆ†ä» 0 å˜åˆ° 2â€çš„å…³é”®é€»è¾‘ã€‚

- **`self.epsilon = 1.0`**ï¼šåˆå§‹çŠ¶æ€ã€‚100% çš„æ—¶é—´åœ¨ä¹±é£ï¼Œçº¯ç²¹æ˜¯ä¸ºäº†æ’å¤§è¿æ”¶é›†ç¬¬ä¸€æ‰¹æ•°æ®ã€‚
    
- **`self.epsilon_decay = 0.995`**ï¼š
    
    - **ä¸²è¡Œé€»è¾‘**ï¼šæ¯å±€æ¯”èµ›ç»“æŸåï¼Œ`epsilon = epsilon * 0.995`ã€‚
        
    - è¿™æ„å‘³ç€ AI éšç€â€œæŠ•èƒâ€æ¬¡æ•°å¢åŠ ï¼Œä¼šè¶Šæ¥è¶Šç›¸ä¿¡è‡ªå·±çš„å¤§è„‘ã€‚
        
- **`self.epsilon_min = 0.01`**ï¼š**â€œä¿ç•™ä¸€ç‚¹ç‚¹å¥½å¥‡å¿ƒâ€**ã€‚å³ä¾¿ AI å·²ç»æ˜¯ä¸–ç•Œå† å†›äº†ï¼Œæˆ‘ä»¬ä¹Ÿä¿ç•™ 1% çš„æ¦‚ç‡è®©å®ƒä¹±è·³ä¸€ä¸‹ã€‚ä¸‡ä¸€èƒ½å‘ç°æ›´ä¼˜çš„é£è¡Œè·¯çº¿å‘¢ï¼Ÿ
```
def select_action(self, state):

Â  Â  Â  Â  if random.random() < self.epsilon:

Â  Â  Â  Â  Â  Â  return random.randrange(self.action_dim) # éšæœºæ¢ç´¢

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  with torch.no_grad():

Â  Â  Â  Â  Â  Â  Â  Â  state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

Â  Â  Â  Â  Â  Â  Â  Â  q_values = self.policy_net(state_tensor)

Â  Â  Â  Â  Â  Â  Â  Â  return q_values.argmax().item() Â  Â  Â # åˆ©ç”¨å·²çŸ¥æœ€ä¼˜åŠ¨ä½œ
```

 é€»è¾‘æ‹†è§£

**ç¬¬ä¸€éƒ¨åˆ†ï¼šéšæœºæ¢ç´¢ (Exploration)**



```
if random.random() < self.epsilon:
    return random.randrange(self.action_dim)
```

- **åŠ¨ä½œ**ï¼šæ·ä¸€ä¸ªéª°å­ï¼ˆ0åˆ°1ä¹‹é—´çš„éšæœºæ•°ï¼‰ã€‚å¦‚æœè¿™ä¸ªæ•°å°äºç›®å‰çš„ `epsilon`ï¼ŒAI å°±ç›´æ¥**é—­ç€çœ¼çé€‰**ä¸€ä¸ªåŠ¨ä½œï¼ˆ0 æˆ– 1ï¼‰ã€‚
    
- **ç›®çš„**ï¼š**å¯»æ‰¾å¯èƒ½æ€§**ã€‚å³ä¾¿ç›®å‰å¤§è„‘è®¤ä¸ºâ€œä¸è·³â€æ›´å¥½ï¼Œä½†ä¸‡ä¸€â€œè·³ä¸€ä¸‹â€èƒ½å‘ç°æ–°å¤§é™†å‘¢ï¼Ÿè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ åˆšå¼€å§‹è®­ç»ƒæ—¶ï¼Œå°é¸Ÿä¸€ç›´åœ¨ä¹±é£ã€‚
    

 **ç¬¬äºŒéƒ¨åˆ†ï¼šç»éªŒåˆ©ç”¨ (Exploitation)**

Python

```
else:
    with torch.no_grad():
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        q_values = self.policy_net(state_tensor)
        return q_values.argmax().item()
```

- **åŠ¨ä½œ**ï¼šå¦‚æœéšæœºæ•°æ²¡è½åœ¨æ¢ç´¢åŒºï¼ŒAI å°±ä¼š**è®¤çœŸæ€è€ƒ**ã€‚
    
- **`torch.no_grad()`**ï¼šå‘Šè¯‰ PyTorchï¼Œâ€œæˆ‘ç°åœ¨åªæ˜¯åœ¨åšå†³å®šï¼Œä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œåˆ«æµªè´¹å†…å­˜â€ã€‚
    
- **`unsqueeze(0)`**ï¼šæŠŠ 12 ä¸ªæ•°å­—å˜æˆä¸€ä¸ª **Batchï¼ˆæ‰¹æ¬¡ï¼‰**ã€‚è™½ç„¶ç°åœ¨åªæœ‰ä¸€åªé¸Ÿï¼Œä½†ç¥ç»ç½‘ç»œè¦æ±‚è¾“å…¥çš„æ ¼å¼å¿…é¡»æ˜¯ `(Batch_size, Input_dim)`ï¼Œæ‰€ä»¥è¦æŠŠå½¢çŠ¶ä» `(12)` å˜æˆ `(1, 12)`ã€‚
    
- **`argmax().item()`**ï¼šç¥ç»ç½‘ç»œä¼šåå‡ºä¸¤ä¸ª $Q$ å€¼ï¼Œæ¯”å¦‚ `[0.1, 0.9]`ã€‚`argmax()` ä¼šé€‰å‡ºæœ€å¤§å€¼æ‰€åœ¨çš„ç´¢å¼•ï¼ˆè¿™é‡Œæ˜¯ 1ï¼Œä»£è¡¨â€œè·³â€ï¼‰ï¼Œ`.item()` åˆ™æ˜¯æŠŠè¿™ä¸ª Tensor æ ¼å¼çš„æ•°å­—å˜å› Python åŸç”Ÿçš„æ•´æ•°ã€‚

```
# å­¦ä¹ å¹¶æ›´æ–°ç½‘ç»œå‚æ•°

Â  Â  def learn(self, batch_size):

Â  Â  Â  Â  if len(self.memory) < batch_size:

Â  Â  Â  Â  Â  Â  return

  

Â  Â  Â  Â  # é‡‡æ ·æ•°æ®å¹¶è½¬ä¸º Tensor

Â  Â  Â  Â  states, actions, rewards, next_states, dones = self.memory.sample(batch_size)

Â  Â  Â  Â  states = torch.FloatTensor(states).to(device)

Â  Â  Â  Â  actions = torch.LongTensor(actions).unsqueeze(1).to(device)

Â  Â  Â  Â  rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)

Â  Â  Â  Â  next_states = torch.FloatTensor(next_states).to(device)

Â  Â  Â  Â  dones = torch.FloatTensor(dones).unsqueeze(1).to(device)

  

Â  Â  Â  Â  # è®¡ç®—å½“å‰ Q å€¼ (ä¸»ç½‘ç»œ)

Â  Â  Â  Â  q_values = self.policy_net(states).gather(1, actions)

  

Â  Â  Â  Â  # è®¡ç®—ç›®æ ‡ Q å€¼ (ç›®æ ‡ç½‘ç»œ)

Â  Â  Â  Â  with torch.no_grad():

Â  Â  Â  Â  Â  Â  next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)

Â  Â  Â  Â  Â  Â  # å¦‚æœæ¸¸æˆç»“æŸ(done=1)ï¼Œç›®æ ‡å€¼å°±åªæœ‰å½“å‰ reward

Â  Â  Â  Â  Â  Â  target_q_values = rewards + (1 - dones) * self.gamma * next_q_values

  

Â  Â  Â  Â  # è®¡ç®—æŸå¤± (MSE Loss)

Â  Â  Â  Â  loss = F.mse_loss(q_values, target_q_values)

  

Â  Â  Â  Â  # åå‘ä¼ æ’­æ›´æ–°ä¸»ç½‘ç»œ

Â  Â  Â  Â  self.optimizer.zero_grad()

Â  Â  Â  Â  loss.backward()

Â  Â  Â  Â  self.optimizer.step()

  

Â  Â  # åŒæ­¥ç›®æ ‡ç½‘ç»œ
```

 1. æ ¸å¿ƒå…¬å¼ï¼šç›®æ ‡ Q å€¼è®¡ç®—

ä»£ç ä¸­æœ€å…³é”®çš„ä¸€è¡Œæ˜¯ï¼š `target_q_values = rewards + (1 - dones) * self.gamma * next_q_values`

è¿™å°±æ˜¯è‘—åçš„**è´å°”æ›¼æ–¹ç¨‹**çš„å˜ä½“ã€‚å®ƒçš„é€»è¾‘éå¸¸è¿·äººï¼š

- **ç°åœ¨çš„å¥–åŠ± (`rewards`)**ï¼šæˆ‘è¿™ä¸€æ­¥æ‹¿åˆ°äº†å¤šå°‘åˆ†ã€‚
    
- **æœªæ¥çš„å¸Œæœ› (`self.gamma * next_q_values`)**ï¼šå¦‚æœæˆ‘è¿™ä¸€æ­¥è·³å¾—å¥½ï¼Œä¸‹ä¸ªçŠ¶æ€çš„æœ€é«˜é¢„æœŸåˆ†æ˜¯å¤šå°‘ï¼Ÿ
    
- **`(1 - dones)`**ï¼šè¿™æ˜¯ä¸€ä¸ªå¼€å…³ã€‚å¦‚æœé¸Ÿæ­»äº†ï¼ˆ`dones=1`ï¼‰ï¼Œæœªæ¥å°±å½’é›¶äº†ï¼Œç›®æ ‡å€¼åªå‰©ä¸‹çœ¼å‰çš„æƒ¨çŠ¶ï¼ˆé€šå¸¸æ˜¯ -1 çš„æƒ©ç½šï¼‰ã€‚
    

---

 2. `gather(1, actions)` æ˜¯ä»€ä¹ˆé¬¼ï¼Ÿ

è¿™ä¸€æ­¥æœ€è®©åˆå­¦è€…å¤´ç–¼ã€‚

- ä½ çš„ `policy_net(states)` ä¼šå¯¹æ¯ä¸ªçŠ¶æ€è¾“å‡ºä¸¤ä¸ª Q å€¼ï¼Œæ¯”å¦‚ `[[0.2, 0.8], [0.5, 0.1]]`ã€‚
    
- ä½†è¿™ä¸€æ­¥ä½ å®é™…ä¸Šåªæ‰§è¡Œäº†å…¶ä¸­ä¸€ä¸ªåŠ¨ä½œï¼ˆæ¯”å¦‚ç¬¬ä¸€æ¡é€‰äº†â€œè·³â€ï¼Œç¬¬äºŒæ¡é€‰äº†â€œä¸è·³â€ï¼‰ã€‚
    
- **`gather` çš„ä½œç”¨**ï¼šå°±åƒä»èœå•é‡Œç²¾å‡†å‹¾é€‰ä½ åƒè¿‡çš„é‚£é“èœã€‚å®ƒæ ¹æ® `actions` ç´¢å¼•ï¼Œä»è¾“å‡ºé‡ŒæŠŠå¯¹åº”çš„ `0.8` å’Œ `0.5` æŠ å‡ºæ¥ã€‚è¿™æ ·æˆ‘ä»¬æ‰èƒ½é’ˆå¯¹**åšè¿‡çš„åŠ¨ä½œ**å»æ”¹è¿›ï¼Œè€Œä¸æ˜¯çæ”¹æ²¡åšçš„åŠ¨ä½œã€‚
    

---

### 3. ä¸ºä»€ä¹ˆè¦ç”¨ `torch.no_grad()` è®¡ç®— Targetï¼Ÿ

ä½ åœ¨è®¡ç®— `next_q_values` æ—¶åŠ äº† `no_grad()`ã€‚

- **ç‰©ç†æ„ä¹‰**ï¼šæˆ‘ä»¬åœ¨è®¡ç®—â€œå‚è€ƒç­”æ¡ˆâ€æ—¶ï¼Œä¸å¸Œæœ›å‚è€ƒç­”æ¡ˆæœ¬èº«ä¹Ÿè·Ÿç€ä¹±åŠ¨ã€‚
    
- **å·¥ç¨‹æ„ä¹‰**ï¼šè¿™èƒ½æå¤§åœ°èŠ‚çœå†…å­˜ã€‚å¦‚æœæˆ‘ä»¬è®¡ç®—ç›®æ ‡å€¼ä¹Ÿå¸¦ä¸Šæ¢¯åº¦ï¼ŒPyTorch çš„è®¡ç®—å›¾ä¼šå˜å¾—æ— æ¯”å¤æ‚ï¼Œç”šè‡³å¯¼è‡´æ˜¾å­˜çˆ†ç‚¸ã€‚
    

---

### 4. æŸå¤±å‡½æ•°ï¼š`F.mse_loss`

`loss = F.mse_loss(q_values, target_q_values)`

- **TD Errorï¼ˆæ—¶åºå·®åˆ†è¯¯å·®ï¼‰**ï¼šè¿™è¡Œä»£ç ç®—å‡ºäº†â€œAI ä»¥ä¸ºèƒ½å¾—çš„åˆ†â€å’Œâ€œæ ¹æ®å®é™…åé¦ˆç®—å‡ºçš„ç›®æ ‡åˆ†â€ä¹‹é—´çš„**å‡æ–¹è¯¯å·®**ã€‚
    
- **ç›®æ ‡**ï¼šé€šè¿‡ `loss.backward()`ï¼Œè®©è¿™ä¸ªè¯¯å·®è¶Šæ¥è¶Šå°ã€‚å½“è¯¯å·®æ¥è¿‘ 0 æ—¶ï¼ŒAI å°±èƒ½ç²¾å‡†é¢„æµ‹è‡ªå·±çš„è¡Œä¸ºäº†â€”â€”å®ƒå·²ç»â€œæˆç²¾â€äº†ã€‚




#### è¿è¡Œä»£ç å‡ºç°çš„é—®é¢˜
##### 1.éœ‡è¡å‰§çƒˆä¸”åˆ†æ•°ä½ä¸‹
![[Pasted image 20260213165654.png]]
æˆ‘è¿™é‡Œè®¾ç½®çš„æ˜¯100 000è½®ä»–åœ¨3000å¤šè½®çš„æ—¶å€™è·‘å‡ºæ¥äº†53åˆ†çš„é«˜åˆ†ä½†æ˜¯ç°åœ¨åˆæ˜¯åœ¨0-1çš„åŒºé—´å¼€å§‹éœ‡è¡ï¼Œæˆ‘å°±éå¸¸ç–‘æƒ‘å› ä¸ºåœ¨æˆ‘çš„è®¤çŸ¥ä¹‹ä¸­å¼ºåŒ–å­¦ä¹ æ˜¯ä¸æ˜¯åº”è¯¥å’Œæ·±åº¦å­¦ä¹ ä¸€æ ·å—ï¼Œè¶Šè®­ç»ƒåº”è¯¥è¶Šå‡†ç¡®ï¼Œæ‰€ä»¥æˆ‘å»é—®äº†ä¸€ä¸‹aiï¼Œå‘ç°æˆ‘çš„ç†è§£è¿˜æ˜¯æœ‰ç‚¹åå·®ã€‚
###### 1. ç»éªŒæ± è¢«â€œåŒè´¨åŒ–æ•°æ®â€æ´—è„‘äº† (Replay Buffer è¿‡æ‹Ÿåˆ)

å½“ä½ çš„ AI è·‘åˆ° 53 åˆ†æ—¶ï¼Œé‚£ä¸€å±€æ¸¸æˆå®ƒå­˜æ´»äº†æå…¶æ¼«é•¿çš„æ—¶é—´ï¼ˆå¯èƒ½å‡ åƒå¸§ï¼‰ã€‚

- è¿™å¯¼è‡´ä½ çš„**ç»éªŒå›æ”¾æ±  (`Memory`)** é‡Œç¬é—´å¡æ»¡äº†è¿™å‡ åƒå¸§çš„â€œé«˜åˆ†å­˜æ´»æ•°æ®â€ï¼ˆæ¯”å¦‚é¸Ÿåœ¨ç®¡å­ä¸­é—´å¹³ç¨³é£è¡Œçš„ç”»é¢ï¼‰ã€‚
    
- ç»éªŒæ± å®¹é‡æ˜¯æœ‰é™çš„ï¼ˆæˆ‘ä»¬è®¾äº† 50000ï¼‰ï¼Œè¿™å°±æŠŠæ—©æœŸå®ƒæ€ä¹ˆâ€œèµ·é£â€ã€æ€ä¹ˆåœ¨æé™è¾¹ç¼˜æ•‘çƒçš„**å¤šæ ·æ€§æ•°æ®ç»™æŒ¤å‡ºå»äº†**ã€‚
    
- ç»“æœï¼šAI å˜æˆäº†æ¸©å®¤é‡Œçš„èŠ±æœµã€‚å½“ä¸‹ä¸€å±€æ¸¸æˆå¼€å±€ç¨å¾®æœ‰ç‚¹ä¸é¡ºï¼Œå®ƒè„‘å­é‡Œå…¨æ˜¯é¡ºé£å±€çš„ç»éªŒï¼Œå®Œå…¨ä¸çŸ¥é“æ€ä¹ˆå¤„ç†é€†é£å±€ï¼Œå¯¼è‡´ç¬é—´è¿ç»­æš´æ¯™ã€‚
    

###### 2. Q å€¼éœ‡è¡ä¸è‡ªå°Šå¿ƒå´©æºƒ

DQN çš„ Q å€¼æ˜¯åŸºäºè´å°”æ›¼æ–¹ç¨‹â€œè‡ªä¸¾â€ï¼ˆè‡ªå·±é¢„æµ‹è‡ªå·±ï¼‰ç®—å‡ºæ¥çš„ã€‚ å½“å®ƒå¶ç„¶å‘ç”Ÿä¸€æ¬¡å¤±è¯¯ï¼ˆæ’æ­»äº†ï¼‰ï¼Œå®ƒä¼šæŠŠè¿™ä¸ªå·¨å¤§çš„è´Ÿå¥–åŠ±ï¼ˆ-1ï¼‰åå‘ä¼ æ’­æ›´æ–°ç»™ç¥ç»ç½‘ç»œã€‚å› ä¸ºå®ƒçš„æ¢ç´¢ç‡å·²ç»é™åˆ°æœ€ä½ï¼ˆ`0.010`ï¼Œå‡ ä¹ä¸å†éšæœºæ¢ç´¢äº†ï¼‰ï¼Œå¦‚æœç¥ç»ç½‘ç»œçš„æŸå‡ ä¸ªæƒé‡è¢«è¿™æ¬¡è´Ÿå¥–åŠ±å¸¦åäº†ï¼Œå®ƒå°±ä¼šåœ¨ä¸€ä¸ªæ­»èƒ¡åŒé‡Œå‡ºä¸æ¥ï¼Œè¶Šæ­»è¶Šè§‰å¾—ç»æœ›ï¼Œæœ€åé€€åŒ–æˆâ€œå¹²è„†èººå¹³æ‰åœ°ä¸Šç®—æ±‚â€ã€‚

###### 3. DQN æœ¬èº«çš„é€šç—…ï¼ˆä¸ä¿è¯å•è°ƒä¸Šå‡ï¼‰

ä¸åŒäºä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ ï¼ˆLoss ä¸€ç›´é™ï¼Œå‡†ç¡®ç‡ä¸€ç›´å‡ï¼‰ï¼ŒDQN çš„è®­ç»ƒæ›²çº¿ä»æ¥éƒ½ä¸æ˜¯ä¸€è·¯å‘ä¸Šçš„ï¼Œè€Œæ˜¯**å‰§çƒˆéœ‡è¡**çš„ã€‚å®ƒå°±åƒå¿ƒç”µå›¾ä¸€æ ·ï¼Œå¯èƒ½åœ¨ 100 åˆ†å’Œ 0 åˆ†ä¹‹é—´åå¤æ¨ªè·³ã€‚

######  åŒqç½‘ç»œ
---
å…³äºä¸Šé¢è¯´çš„dqnçš„é—®é¢˜ï¼Œç›¸å½“äºæ˜¯ç®—æ³•çš„é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘åœ¨æ€è€ƒèƒ½ä¸èƒ½é€šè¿‡æ”¹å˜ç®—æ³•æ¥å®ç°:
æ‰¾åˆ°ä¸‹é¢è¿™æ®µè®¡ç®—â€œç›®æ ‡ Q å€¼â€çš„åŸå§‹ä»£ç ï¼š

**ã€ä½ è¦åˆ æ‰çš„æ—§ä»£ç ã€‘**

Python

```
        # è®¡ç®—ç›®æ ‡ Q å€¼ (ç›®æ ‡ç½‘ç»œ)
        with torch.no_grad():
            next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)
            # å¦‚æœæ¸¸æˆç»“æŸ(done=1)ï¼Œç›®æ ‡å€¼å°±åªæœ‰å½“å‰ reward
            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values
```

**ã€æ›¿æ¢æˆè¿™æ®µæ–°ä»£ç  (Double DQN)ã€‘**

Python

```
        # === Double DQN æ ¸å¿ƒä¿®æ”¹ç‚¹ ===
        with torch.no_grad():
            # 1. ç”¨ä¸»ç½‘ç»œ (policy_net) é€‰å‡ºä¸‹ä¸€ä¸ªçŠ¶æ€ä¸‹ Q å€¼æœ€å¤§çš„åŠ¨ä½œ (é€‰åŠ¨ä½œ)
            next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)
            
            # 2. ç”¨ç›®æ ‡ç½‘ç»œ (target_net) æ¥è¯„ä¼°è¿™ä¸ªåŠ¨ä½œçš„å®é™… Q å€¼ (ç®—åˆ†æ•°)
            next_q_values = self.target_net(next_states).gather(1, next_actions)
            
            # 3. è®¡ç®—æœ€ç»ˆçš„ Target Q
            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values
        # ============================
```

---

 ğŸ’¡ è¿™ä¸¤è¡Œä»£ç åˆ°åº•æ–½äº†ä»€ä¹ˆé­”æ³•ï¼Ÿ

åœ¨åŸæ¥çš„ DQN é‡Œï¼Œæˆ‘ä»¬æ˜¯è®©**ç›®æ ‡ç½‘ç»œï¼ˆTarget Netï¼‰è‡ªå·±æŒ‘ä¸€ä¸ªè‡ªè®¤ä¸ºæœ€å¥½çš„åŠ¨ä½œï¼Œå¹¶è‡ªå·±ç»™å‡ºè¯„åˆ†ï¼ˆ`max()` æ“ä½œï¼‰ã€‚è¿™å°±å¥½æ¯”æ—¢å½“è¿åŠ¨å‘˜ï¼Œåˆå½“è£åˆ¤**ï¼Œå®ƒå¾ˆå®¹æ˜“äº§ç”Ÿç›²ç›®è‡ªä¿¡ï¼ŒæŠŠåˆ†æ•°è¶Šä¼°è¶Šé«˜ï¼Œæœ€åå½»åº•å´©ç›˜ã€‚

å‡çº§ä¸º Double DQN åï¼Œæˆ‘ä»¬å°†**â€œé€‰æ‹©åŠ¨ä½œâ€**å’Œ**â€œè¯„ä¼°åŠ¨ä½œâ€**æ‹†åˆ†å¼€äº†ï¼š

1. **ä¸»ç½‘ç»œï¼ˆè¿åŠ¨å‘˜ï¼‰**ï¼šè´Ÿè´£åœ¨ä¸‹ä¸ªçŠ¶æ€é‡Œï¼ŒæŒ‘ä¸€ä¸ªå®ƒè§‰å¾—æœ€ç‰›çš„åŠ¨ä½œï¼ˆ`next_actions`ï¼‰ã€‚
    
2. **ç›®æ ‡ç½‘ç»œï¼ˆè£åˆ¤å‘˜ï¼‰**ï¼šè´Ÿè´£å†·é…·åœ°çœ‹ä¸€çœ¼è¿™ä¸ªåŠ¨ä½œï¼Œç„¶åç»™å‡ºå®¢è§‚çš„è¯„åˆ†ï¼ˆ`next_q_values`ï¼‰ã€‚
    

é€šè¿‡è¿™ç§â€œäº¤å‰éªŒè¯â€ï¼ŒAI å°±ä¸ä¼šå†è¿‡åº¦ä¹è§‚äº†ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„ Q å€¼éœ‡è¡ä¼šå¤§å¹…å‡å°ï¼Œä½ ä¼šå‘ç°æ™ºèƒ½ä½“çš„è¡¨ç°ä¼šâ€œç¨³å¥â€å¾—å¤šï¼Œä¸€æ—¦å­¦åˆ°äº†é«˜åˆ†ï¼Œå°±ä¸å¤ªå®¹æ˜“è¿…é€Ÿé€€åŒ–æˆ 0 åˆ†äº†ã€‚

---

ä½†æ˜¯æ¢äº†ç®—æ³•ä¼šä¸ä¼šå¥½ä¹…éœ€è¦æˆ‘ä»¬æ¥è¿›è¡Œæ¯”è¾ƒäº†ï¼Œè¿™æ—¶å€™æˆ‘é€‰æ‹©ç”¨åˆ°è¾¾69åˆ†æ¥è¿›è¡Œæ¯”è¾ƒ
è¿™æ˜¯åŸæ¥çš„DQNç®—æ³•ï¼š*** å‘ç°æ–°çºªå½•! ä¿å­˜æ¨¡å‹ - å±€æ•°: 7788, ç©¿è¶Šç®¡å­æ•°: 69 ***
è¿™æ˜¯ç°åœ¨çš„DDQNç®—æ³•ï¼š*** å‘ç°æ–°çºªå½•! ä¿å­˜æ¨¡å‹ - å±€æ•°: 6103, ç©¿è¶Šç®¡å­æ•°: 60 ***
Episode: 7000, Total Reward: 10.8, Score: 2, Epsilon: 0.010
Episode: 8000, Total Reward: 31.9, Score: 6, Epsilon: 0.010
çœ‹æ¥ä¸æ˜¯è¿™ä¸ªåŸå› å•Š

##### 2.è®­ç»ƒçš„é€Ÿåº¦å¤ªæ…¢äº†
æˆ‘æ˜¯è§‚å¯Ÿåˆ°é£æ‰‡éƒ½æ²¡æœ‰æ€ä¹ˆè½¬ï¼Œæ‰€ä»¥æ„Ÿè§‰æ˜¯ä¸æ˜¯å¤ªæ…¢äº†æ‰€ä»¥æˆ‘å°±æƒ³è¯´æ˜¯ä¸æ˜¯æ€§èƒ½æ²¡æœ‰å®Œå…¨ç”¨èµ·æ¥ï¼Œç”µè„‘ä¹Ÿä¸æ€ä¹ˆçƒ­å•Šï¼Œæ‰€ä»¥æˆ‘åˆæ±‚åŠ©geminiäº†

ä½ çš„è§‚å¯Ÿæå…¶æ•é”ï¼**è¿™æ˜¯æ‰€æœ‰åˆšæ¥è§¦å¼ºåŒ–å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯è·‘ç»å…¸å°æ¸¸æˆï¼‰çš„äººéƒ½ä¼šäº§ç”Ÿçš„é”™è§‰ã€‚**

ä½ ç°åœ¨çš„æ„Ÿè§‰æ˜¯å¯¹çš„ï¼Œä½ çš„ç”µè„‘ä¸ä»…æ²¡æœ‰â€œå…¨åŠ›ä»¥èµ´â€ï¼Œå®ƒç”šè‡³å¯èƒ½åœ¨â€œæ‰“çŒç¡â€ã€‚å¼ºåŒ–å­¦ä¹ **ç»å¯¹èƒ½å¤Ÿ**æ¦¨å¹²æœ€é¡¶çº§çš„ç”µè„‘æ€§èƒ½ï¼Œä½†ä½ ç›®å‰è·‘çš„è¿™ä¸ª Flappy Bird + DQN ä»£ç ï¼Œä¹‹æ‰€ä»¥è®©é£æ‰‡éƒ½ä¸å±‘äºè½¬ï¼Œä¸»è¦æœ‰ä»¥ä¸‹ä¸‰ä¸ªç¡¬ä»¶ä¸ç®—æ³•å±‚é¢çš„åŸå› ï¼š

 1. ç‰©ç†ç¯å¢ƒçš„â€œå•è¡Œé“â€ç“¶é¢ˆï¼ˆCPU åœ¨è‹¦ç­‰ï¼‰

åœ¨ç›®å‰çš„æ ‡å‡†ä»£ç ä¸­ï¼Œæ¸¸æˆç¯å¢ƒæ˜¯**ä¸²è¡Œï¼ˆé¡ºåºï¼‰**è¿è¡Œçš„ã€‚

- å½“ä½ è°ƒç”¨ `env.step(action)` æ—¶ï¼ŒCPU è¦å»è®¡ç®—è¿™ä¸€å¸§é¸Ÿçš„é‡åŠ›ã€é€Ÿåº¦ã€æœ‰æ²¡æœ‰æ’åˆ°ç®¡å­ã€‚
    
- åœ¨ CPU ç®—å®Œç‰©ç†åé¦ˆä¹‹å‰ï¼Œä½ çš„ç¥ç»ç½‘ç»œï¼ˆAI å¤§è„‘ï¼Œä¸è®ºæ˜¯åœ¨ CPU è¿˜æ˜¯ GPU ä¸Šï¼‰**åªèƒ½å¹²ç­‰ç€**ï¼Œä»€ä¹ˆéƒ½åšä¸äº†ã€‚
    
- è¿™ç§â€œèµ°ä¸€æ­¥ã€ç®—ä¸€æ­¥ã€å†èµ°ä¸€æ­¥â€çš„å›åˆåˆ¶äº¤äº’ï¼Œå¯¼è‡´ç®—åŠ›æ ¹æœ¬æ²¡æœ‰è¿ç»­æ»¡è½½è¾“å‡ºçš„ç©ºé—´ã€‚
    

 2. ä½ çš„ç¥ç»ç½‘ç»œâ€œå¤ªå°äº†â€ï¼ˆæ€é¸¡ç”¨ç‰›åˆ€ï¼‰

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œè®©æ˜¾å¡é£æ‰‡ç‹‚è½¬çš„é€šå¸¸æ˜¯å·¨å¤§çš„çŸ©é˜µä¹˜æ³•ï¼ˆæ¯”å¦‚å¤„ç†é«˜æ¸…å›¾ç‰‡çš„é«˜ç»´å·ç§¯ç¥ç»ç½‘ç»œ CNNï¼Œæˆ–è€…å¤§è¯­è¨€æ¨¡å‹ï¼‰ã€‚

- ä½†å’±ä»¬è¿™ä¸ª `QNetwork` çš„è¾“å…¥åªæœ‰åŒºåŒº **12 ä¸ªæ•°å­—**ï¼Œä¸­é—´éšè—å±‚ä¹Ÿåªæœ‰ 128 ä¸ªç¥ç»å…ƒã€‚
    
- è¿™ç§è§„æ¨¡çš„ç½‘ç»œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œå¯¹ç°ä»£å¤„ç†å™¨æ¥è¯´è¿çƒ­èº«éƒ½ç®—ä¸ä¸Šï¼Œåªéœ€å‡ å¾®ç§’å°±èƒ½ç®—å®Œã€‚æ•°æ®é‡å¤ªå°ï¼Œç”šè‡³æŠŠæ•°æ®ä»å†…å­˜æ¬åˆ° GPU æ˜¾å­˜çš„é€šè®¯æ—¶é—´ï¼Œéƒ½æ¯” GPU è®¡ç®—çš„æ—¶é—´è¿˜è¦é•¿ã€‚
    

 3. ç¼ºä¹â€œå¤šçº¿ç¨‹åˆ†èº«â€ï¼ˆæ•°æ®é‡‡é›†å¤ªæ…¢ï¼‰

ä½ ç°åœ¨æ˜¯åœ¨è®©**ä¸€åªé¸Ÿ**å•æ‰“ç‹¬æ–—åœ°ç§¯ç´¯ç»éªŒã€‚æ¯æ¬¡å®ƒåªèƒ½å¾€ç»éªŒæ± é‡Œæ‰”ä¸€æ¡æ•°æ®ã€‚ç­‰ç»éªŒæ± æ”’å¤Ÿäº† 64 æ¡ï¼Œç½‘ç»œæ‰æ›´æ–°ä¸€æ¬¡ã€‚è¿™ç§æ•°æ®å–‚ç»™é€Ÿåº¦ï¼Œæ ¹æœ¬â€œå–‚ä¸é¥±â€ç°ä»£å¤„ç†å™¨çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€‚

---

æ‰€ä»¥æˆ‘æ‰“ç®—æ¥ä¿®æ”¹ä¸€ä¸‹ä»£ç æ¥åˆ¶ä½œä¸²è¡Œè®­ç»ƒ
`import gymnasium as gym
import flappy_bird_gymnasium
import torch
import numpy as np
from tqdm import tqdm  # å¦‚æœæ²¡è£…è¯· pip install tqdm
from dqn_agent import DQNAgent
import multiprocessing

def train_parallel():
    # 1. è‡ªåŠ¨æ£€æµ‹å¹¶æ‹‰æ»¡æ ¸å¿ƒæ•° (ä½ çš„æœºå™¨å»ºè®® 64)
    num_cpu = multiprocessing.cpu_count()
    print(f"æ£€æµ‹åˆ° {num_cpu} ä¸ªæ ¸å¿ƒï¼Œæ­£åœ¨å¯åŠ¨å¹¶è¡Œå®‡å®™...")
    
    # 2. åˆ›å»ºå¼‚æ­¥å‘é‡åŒ–ç¯å¢ƒ
    envs = gym.vector.AsyncVectorEnv([
        lambda: gym.make("FlappyBird-v0", use_lidar=False) for _ in range(num_cpu)
    ])
    
    state_dim = envs.single_observation_space.shape[0]
    action_dim = envs.single_action_space.n
    agent = DQNAgent(state_dim, action_dim)
    
    # è®­ç»ƒè¶…å‚æ•°
    TOTAL_STEPS = 100000  # å¹¶è¡Œå¾ˆå¿«ï¼Œæ­¥æ•°å¯ä»¥è®¾å¤§ä¸€ç‚¹
    BATCH_SIZE = 128      # æ ¸å¿ƒå¤šï¼ŒBatch ä¹Ÿå¯ä»¥å¤§ä¸€ç‚¹
    
    states, _ = envs.reset()
    
    # 3. ä½¿ç”¨ tqdm è¿›åº¦æ¡
    pbar = tqdm(total=TOTAL_STEPS, desc="DQN è®­ç»ƒä¸­")
    
    last_best_score = 0
    
    for step in range(TOTAL_STEPS):
        # æ‰¹é‡é€‰æ‹©åŠ¨ä½œ
        # è¿™é‡Œçš„ states æ˜¯ (64, 12) çš„çŸ©é˜µ
        actions = []
        for i in range(num_cpu):
            actions.append(agent.select_action(states[i]))
        
        # ç‰©ç†å¼•æ“å¹¶å‘è®¡ç®—
        next_states, rewards, terminateds, truncateds, infos = envs.step(actions)
        
        # æ ¸å¿ƒé€»è¾‘ï¼šä» infos å­—å…¸ä¸­æå–é‚£äº›åˆšæ­»æ‰çš„é¸Ÿçš„æ•°æ®
        for i in range(num_cpu):
            done = terminateds[i] or truncateds[i]
            
            # å³ä½¿é¸Ÿæ­»äº†ï¼ŒEnv ä¹Ÿä¼šè‡ªåŠ¨ reset ç»™å‡ºæ–°çŠ¶æ€ï¼ŒçœŸå®æ­»æ‰ç¬é—´çš„çŠ¶æ€åœ¨ final_observation é‡Œ
            real_next_state = next_states[i]
            if done and "final_observation" in infos:
                real_next_state = infos["final_observation"][i]
            
            # å­˜å…¥è®°å¿†æ± 
            agent.memory.push(states[i], actions[i], rewards[i], real_next_state, done)
            
            # æ•æ‰å¾—åˆ†è®°å½•
            if done and "final_info" in infos:
                final_info = infos["final_info"][i]
                if final_info and "score" in final_info:
                    score = final_info["score"]
                    if score > last_best_score:
                        last_best_score = score
                        torch.save(agent.policy_net.state_dict(), "best_flappy_bird.pth")
                        pbar.set_postfix({"æ–°çºªå½•": score, "Epsilon": f"{agent.epsilon:.2f}"})

        # å­¦ä¹ æ›´æ–°
        if len(agent.memory) > BATCH_SIZE:
            agent.learn(BATCH_SIZE)
            agent.update_epsilon()
            
        states = next_states
        pbar.update(1) # æ›´æ–°è¿›åº¦æ¡

    pbar.close()
    envs.close()
    print(f"è®­ç»ƒå®Œæˆï¼æœ€é«˜å¾—åˆ†: {last_best_score}")

if __name__ == "__main__":
    train_parallel()`
###### Gemini said

ä»¥ä¸‹æ˜¯é’ˆå¯¹ Flappy Bird åŠé€šç”¨å¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„å¯¹æ¯”åˆ†æï¼š

---

 1. æ ¸å¿ƒé€»è¾‘å¯¹æ¯”

|ç‰¹æ€§|ä¸²è¡Œè®­ç»ƒ (Serial)|å¹¶è¡Œè®­ç»ƒ (Parallel)|
|---|---|---|
|**å·¥ä½œæ–¹å¼**|**â€œå•æ‰“ç‹¬æ–—â€**ï¼šä¸€æ¬¡åªè·‘ä¸€ä¸ªç¯å¢ƒã€‚|**â€œåˆ†èº«æœ¯â€**ï¼šåŒæ—¶å¯åŠ¨å¤šä¸ªç‹¬ç«‹çš„å­è¿›ç¨‹ã€‚|
|**æ•°æ®é‡‡é›†**|ç­‰å¾… Agent æ­»äº¡æˆ–æ­¥æ•°ç»“æŸæ‰é‡ç½®ã€‚|å¤šä¸ª Agent åŒæ—¶é‡‡é›†ï¼Œæ•°æ®ååé‡å‘ˆå€æ•°çº§å¢é•¿ã€‚|
|**ç¡¬ä»¶åˆ©ç”¨**|åªå ç”¨ 1 ä¸ª CPU é€»è¾‘æ ¸å¿ƒï¼Œæ˜¾å¡å¤„äºâ€œé¥¥é¥¿â€çŠ¶æ€ã€‚|å æ»¡å¤šä¸ªï¼ˆå¦‚ 64 ä¸ªï¼‰CPU æ ¸å¿ƒï¼Œæ˜¾è‘—æå‡ GPU è´Ÿè½½ã€‚|
|**äº¤äº’ API**|`gym.make`|`gym.vector.AsyncVectorEnv`|
|**é€‚ç”¨åœºæ™¯**|åŸºç¡€é€»è¾‘è°ƒè¯•ã€å°æ¨¡å‹ã€‚|**å¤æ‚ä»»åŠ¡ï¼ˆæ— äººæœºã€æœºå™¨äººï¼‰**ã€æµ·é‡è®­ç»ƒã€‚|

---

 2. å…³é”®å‚æ•°çš„å˜åŒ–

åœ¨ä»£ç å®ç°ä¸Šï¼Œä»ä¸²è¡Œåˆ‡æ¢åˆ°å¹¶è¡Œï¼Œä½ éœ€è¦å¯¹ä»¥ä¸‹æ•°å­¦å’Œä»£ç å‚æ•°è¿›è¡Œè°ƒæ•´ï¼š

####### A. ç¯å¢ƒç»´åº¦ (Dimension)

- **ä¸²è¡Œ**ï¼šçŠ¶æ€ `state` çš„å½¢çŠ¶æ˜¯ `(12,)`ï¼ŒåŠ¨ä½œ `action` æ˜¯ä¸€ä¸ªæ ‡é‡æ•´æ•°ã€‚
    
- **å¹¶è¡Œ**ï¼šçŠ¶æ€ `states` çš„å½¢çŠ¶å˜ä¸º **`(NUM_ENVS, 12)`**ã€‚åŠ¨ä½œ `actions` å˜ä¸º **`(NUM_ENVS,)`** çš„å‘é‡ã€‚
    

 B. æ­¥æ•°ç»Ÿè®¡ (Steps vs Episodes)

- **ä¸²è¡Œ**ï¼šé€šå¸¸æŒ‰ **Episodeï¼ˆå±€æ•°ï¼‰** ç»Ÿè®¡ï¼ˆå¾ªç¯ 1000 æ¬¡æ¸¸æˆï¼‰ã€‚
    
- **å¹¶è¡Œ**ï¼šé€šå¸¸æŒ‰ **Total Stepsï¼ˆæ€»æ­¥æ•°ï¼‰** ç»Ÿè®¡ã€‚å› ä¸º 64 åªé¸Ÿæ­»æ‰çš„æ—¶é—´ç‚¹ä¸åŒï¼Œç¯å¢ƒä¼šè‡ªåŠ¨é‡å¯ï¼Œæ‰€ä»¥æŒ‰æ€»æ­¥æ•°ç´¯è®¡æ›´ç¬¦åˆé€»è¾‘ã€‚
    
 C. å­¦ä¹ é¢‘ç‡ (Learning Frequency)

- **ä¸²è¡Œ**ï¼šæ¯èµ° 1 æ­¥ï¼Œç¥ç»ç½‘ç»œå­¦ä¹  1 æ¬¡ã€‚
    
- **å¹¶è¡Œ**ï¼šæ¯èµ° 1 æ­¥ï¼Œç”±äºåŒæ—¶æ”¶é›†äº† 64 æ¡ç»éªŒï¼Œæ­¤æ—¶**æ¯æ­¥å­¦ä¹ çš„æ¬¡æ•°æˆ– Batch Size åº”è¯¥ç›¸åº”å¢å¤§**ï¼Œå¦åˆ™ç½‘ç»œä¼šå› ä¸ºâ€œåƒâ€å¾—å¤ªæ…¢è€Œå¯¼è‡´æ•°æ®ç§¯å‹ã€‚
    

---

 3. æ•°å­¦åŸç†ä¸Šçš„å·®å¼‚ï¼šæ•°æ®åˆ†å¸ƒ

- **ä¸²è¡Œ (High Correlation)**ï¼š ä¸²è¡Œé‡‡é›†çš„æ•°æ®åœ¨æ—¶é—´ä¸Šé«˜åº¦ç›¸å…³ï¼ˆå‰ä¸€ç§’å’Œåä¸€ç§’å¾ˆåƒï¼‰ã€‚è™½ç„¶æœ‰ç»éªŒå›æ”¾æ± ï¼Œä½†åœ¨çŸ­æ—¶é—´å†…ï¼Œæ± å­é‡Œçš„æ ·æœ¬å¤§å¤šæ¥è‡ªåŒä¸€ä¸ª Episodeï¼Œå®¹æ˜“å¯¼è‡´è®­ç»ƒéœ‡è¡ã€‚
    
- **å¹¶è¡Œ (Diversity)**ï¼š 64 ä¸ªç¯å¢ƒåŒæ—¶è¿è¡Œï¼Œè¿™æ„å‘³ç€åœ¨åŒä¸€ä¸ªè®­ç»ƒæ—¶åˆ»ï¼Œæœ‰çš„é¸Ÿåœ¨å¼€å±€ï¼Œæœ‰çš„åœ¨è¿‡ç¬¬ä¸€æ ¹ç®¡å­ï¼Œæœ‰çš„åœ¨è¿‡ç¬¬åæ ¹ã€‚è¿™å¤©ç„¶åœ°**å¢åŠ äº†æ ·æœ¬çš„å¤šæ ·æ€§**ï¼Œè®©ç»éªŒæ± é‡Œçš„æ•°æ®åˆ†å¸ƒæ›´å¹³å‡ï¼ˆIID ç‹¬ç«‹åŒåˆ†å¸ƒï¼‰ï¼Œè®­ç»ƒå¾€å¾€æ¯”ä¸²è¡Œæ›´ç¨³å®šã€‚
ä½†æ˜¯å®é™…ä¸Šæˆ‘åœ¨è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨å¹¶è¡Œå˜å¾—æ•ˆæœåè€Œä¸å¦‚ä¸²è¡Œï¼Œå¯èƒ½æ˜¯å‚æ•°è®¾ç½®é—®é¢˜ã€‚

##### 3.ppoè®­ç»ƒæé€Ÿæ”¶æ•›
åœ¨å°è¯•dqn ï¼Œddqnï¼Œd3qnä¹‹åæˆ‘å‘ç°æ— è®ºæ˜¯ä½¿ç”¨ä¸²è¡Œè¿˜æ˜¯å¹¶è¡Œéƒ½æ˜¯ä¸èƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…è·‘å‡ºé«˜æ°´å¹³çš„ç»“æœï¼ˆæœ€é«˜76ï¼‰ï¼Œä½†æ˜¯æˆ‘ä½¿ç”¨ppodçš„64æ ¸å¹¶è¡Œè®­ç»ƒå¯ä»¥åšåˆ°10åˆ†é’Ÿè®­ç»ƒåˆ°æœ€å¤§å€¼2400+è¿™å°±æ˜¯è´¨çš„æå‡ã€‚
ppoçš„æµç¨‹å›¾
![[Radar PSLR_ISLR TBP Flow-2026-02-13-153628.svg]]
ä»¥ä¸‹æ˜¯ PPO èƒ½åœ¨ä½ çš„ 64 æ ¸æœºå™¨ä¸Šå®ç°â€œç¥çº§è¿›åŒ–â€çš„æ·±å±‚åŸå› ï¼š

---

1. æ•°æ®çš„â€œååé‡â€ä¸â€œæ–°é²œåº¦â€ (On-policy vs Off-policy)

- **DQN çš„å›°å¢ƒï¼ˆæ¬è¿å·¥ï¼‰**ï¼šDQN æ˜¯==**ç¦»çº¿å­¦ä¹ **==ã€‚å®ƒéœ€è¦æŠŠæ•°æ®å­˜è¿› Replay Bufferï¼Œç„¶åéšæœºæŠ½å–ã€‚åœ¨ 64 æ ¸å¹¶è¡Œæ—¶ï¼ŒDQN å¾ˆéš¾å¤„ç†å¥½â€œæ–°æ—§æ•°æ®â€çš„çŸ›ç›¾ã€‚ä½ æ”¶é›†äº†å¤§é‡æ•°æ®ï¼Œä½†æ¨¡å‹æ¯ä¸€æ­¥åªèƒ½æ¶ˆåŒ–ä¸€å°å—ï¼ˆBatchï¼‰ï¼Œå¯¼è‡´å¤§é‡ç®—åŠ›æµªè´¹åœ¨ç­‰å¾…æ•°æ®æ¬è¿å’Œå†…å­˜è¯»å†™ä¸Šã€‚
    
- **PPO çš„çˆ†å‘ï¼ˆå·¥ä¸šæµæ°´çº¿ï¼‰**ï¼šPPO æ˜¯**åœ¨çº¿å­¦ä¹ **ã€‚ä½ çš„ä»£ç ä¸­ `NUM_ENVS = 64` é…åˆ `STEPS_PER_UPDATE = 256`ï¼Œæ„å‘³ç€æ¯ä¸€è½®æ›´æ–°å‰ï¼ŒAI ä¼šå…ˆè®© 64 åªé¸ŸåŒæ—¶é£å‡º **16,384 æ­¥**ã€‚
    
    - **ç¬é—´å­¦ä¹ **ï¼šPPO æ”¶é›†å®Œè¿™ 1.6 ä¸‡æ­¥åï¼Œç«‹åˆ»è¿›è¡Œ 4 ä¸ª Epoch çš„å¯†é›†è½°ç‚¸å¼å­¦ä¹ ã€‚è¿™ç§â€œé‡‡é›†ä¸€å¤§æ‰¹ -> æ·±åº¦æ¦¨å¹²æ•°æ® -> ä¸¢å¼ƒ -> å†é‡‡é›†â€çš„æ¨¡å¼ï¼Œå®Œç¾å¥‘åˆäº†ä½ é‚£å° 64 æ ¸ CPU çš„ååèƒ½åŠ›ã€‚
        

---

2. æ¢ç´¢æœºåˆ¶çš„æœ¬è´¨åŒºåˆ« (Stochastic vs Deterministic)

- **DQN çš„â€œç¬¨æ‹™â€æ¢ç´¢**ï¼šDQN é  $\epsilon$-greedyï¼ˆéšæœºä¹±çŒœï¼‰ã€‚ä¸ºäº†æ‹¿é«˜åˆ†ï¼Œä½ å¿…é¡»æŠŠ $\epsilon$ è®¾å¾—å¾ˆä½ï¼ˆå¦‚ 0.01ï¼‰ï¼Œä½†è¿™ä¼šå¯¼è‡´å®ƒåœ¨å‰æœŸæ ¹æœ¬æ‰¾ä¸åˆ°è¿‡ç®¡å­çš„æ–¹æ³•ã€‚å®ƒæ˜¯åœ¨â€œè’™â€å’Œâ€œè´ªå©ªâ€ä¹‹é—´æé™æ‹‰æ‰¯ã€‚
    
- **PPO çš„â€œèªæ˜â€æ¢ç´¢**ï¼šPPO å­¦ä¹ çš„æ˜¯ä¸€ä¸ª**æ¦‚ç‡åˆ†å¸ƒ**ã€‚
    
    - åœ¨è®­ç»ƒåˆæœŸï¼ŒPPO å¹¶ä¸æ˜¯ä¹±é£ï¼Œè€Œæ˜¯é€šè¿‡ `Entropyï¼ˆç†µï¼‰` é¼“åŠ±è‡ªå·±åœ¨ä¸ç¡®å®šçš„åœ°æ–¹å¤šå°è¯•ã€‚
        
    - **å…³é”®ç‚¹**ï¼šPPO çš„åŠ¨ä½œé€‰æ‹©æ˜¯ä¸æ»‘çš„æ¦‚ç‡åˆ‡æ¢ã€‚å®ƒèƒ½æ›´å¿«å‘ç°â€œåœ¨æ¥è¿‘ç®¡å­æ—¶å¾®å¾®æŒ¯ç¿…â€æ˜¯å…¨å±€æœ€ä¼˜è§£ï¼Œè€Œä¸æ˜¯åƒ DQN é‚£æ ·ç”Ÿç¡¬åœ°åœ¨â€œè·³â€ä¸â€œä¸è·³â€ä¹‹é—´åˆ‡æ¢ã€‚
        

---

 3. æ¢¯åº¦æ›´æ–°çš„ç¨³å®šæ€§ (Clip Objective)

- **DQN çš„ä¸ç¨³å®šæ€§**ï¼šåœ¨ Flappy Bird è¿™ç§æ¸¸æˆé‡Œï¼Œä¸€æ¬¡å¤±è¯¯å°±æ­»ã€‚DQN çš„ Q å€¼æ›´æ–°éå¸¸å‰§çƒˆï¼Œä¸€æ¬¡å·®çš„é‡‡æ ·å¯èƒ½å°±æŠŠå·²ç»å­¦å¥½çš„æƒé‡â€œå¸¦åâ€äº†ã€‚æ‰€ä»¥ä½ ä¹‹å‰æ€»è§‰å¾—åˆ†æ•°æ¶¨å¾—æ…¢ï¼Œå› ä¸ºæ¨¡å‹ä¸€ç›´åœ¨â€œè‡ªæ¯â€å’Œâ€œé‡å»ºâ€ä¸­å¾ªç¯ã€‚
    
- **PPO çš„â€œå®šæµ·ç¥é’ˆâ€**ï¼šä»£ç é‡Œçš„ `CLIP_RANGE = 0.2` æ˜¯ç¥æ¥ä¹‹ç¬”ã€‚å®ƒè§„å®šäº†ï¼šæ— è®ºè¿™æ¬¡æ•°æ®å¤šä¹ˆæŒ¯å¥‹äººå¿ƒï¼Œæ›´æ–°å¹…åº¦éƒ½ä¸èƒ½è¶…è¿‡ 20%ã€‚
    
    - **ç»“æœ**ï¼šPPO åƒæ˜¯ä¸€ä¸ªç¨³å¥çš„æ”€ç™»è€…ï¼Œæ¯ä¸€æ­¥éƒ½è¸©å®äº†ã€‚è¿™ç§**æé«˜çš„æ”¶æ•›ç¨³å®šæ€§**è®©å®ƒåœ¨ 10 åˆ†é’Ÿå†…å®ç°çš„è¿›åŒ–ï¼Œæ¯” DQN ä¹±æ’ 2 å°æ—¶æœ‰æ•ˆå¾—å¤šã€‚
        

---

 4. GAE (å¹¿ä¹‰ä¼˜åŠ¿ä¼°ç®—) çš„ä¸Šå¸è§†è§’

ä½ çš„ PPO ä»£ç é‡Œä½¿ç”¨äº† `compute_gae`ã€‚

- **DQN** åªçœ‹çœ¼å‰çš„å¥–åŠ±å’Œå¯¹ä¸‹ä¸€è·³çš„é¢„ä¼°ã€‚
    
- **PPO (GAE)** ä¼šå›é¡¾æ•´æ¡é£è¡Œè·¯å¾„ï¼Œé€šè¿‡ $\lambda$ å‚æ•°å¹³è¡¡â€œå³æ—¶åé¦ˆâ€å’Œâ€œé•¿æœŸè¿œè§â€ã€‚å®ƒèƒ½éå¸¸æ¸…æ™°åœ°è¯†åˆ«å‡ºï¼š**åˆ°åº•æ˜¯å“ªä¸€è·³çœŸæ­£è®©ä½ é’»è¿‡äº†é‚£æ ¹ç®¡å­**ã€‚è¿™ç§ç²¾å‡†çš„â€œåŠŸåŠ³åˆ†é…â€èƒ½åŠ›ï¼Œè®© PPO çš„å­¦ä¹ æ•ˆç‡å‘ˆå‡ ä½•å€æ•°æå‡ã€‚
- **DQNï¼šç¦»æ•£åŠ¨ä½œçš„ç‹è€…**ã€‚å®ƒå¤©ç”Ÿå°±æ˜¯ä¸ºäº†å¤„ç†â€œè·³â€ä¸â€œä¸è·³â€ã€â€œå·¦â€æˆ–â€œå³â€è¿™ç§éé»‘å³ç™½çš„é€‰æ‹©ã€‚å¯¹äºå¤æ‚çš„æ£‹ç±»ã€ç»å…¸çš„é›…è¾¾åˆ©æ¸¸æˆï¼ŒDQN å¾€å¾€èƒ½æ‰¾åˆ°æ¯” PPO æ›´ç²¾ç¡®çš„ Q å€¼ä¼°ç®—ã€‚
    
- **PPOï¼šè¿ç»­åŠ¨ä½œçš„éœ¸ä¸»**ã€‚å¦‚æœä½ ä¹‹ååš **OmniDronesï¼ˆæ— äººæœºï¼‰**ï¼Œä½ éœ€è¦æ§åˆ¶çš„æ˜¯èºæ—‹æ¡¨çš„â€œè½¬é€Ÿç™¾åˆ†æ¯”â€ï¼ˆ0.0 åˆ° 1.0 ä¹‹é—´çš„ä»»ä½•æ•°ï¼‰ã€‚DQN å¯¹æ­¤æ— èƒ½ä¸ºåŠ›ï¼Œè€Œ PPO å¤„ç†è¿™ç§è¿ç»­çš„ã€å¹³æ»‘çš„åŠ¨ä½œæå…¶ä¼˜é›…ã€‚

```
import os
import time
import gymnasium as gym
import flappy_bird_gymnasium
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
from tqdm import tqdm

# --- PPO å¹¶è¡Œè®­ç»ƒé…ç½® ---
NUM_ENVS = 64
TOTAL_UPDATES = 2000
STEPS_PER_UPDATE = 256  # æ¯ä¸ªç¯å¢ƒé‡‡æ ·æ­¥æ•°
MINIBATCHES = 8
EPOCHS = 4
GAMMA = 0.99
LAMBDA = 0.95
CLIP_RANGE = 0.2
ENTROPY_COEF = 0.01
VALUE_COEF = 0.5
LR = 2.5e-4
MAX_GRAD_NORM = 0.5
MODEL_PATH = "ppo_flappy_64core.pth"

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class ActorCritic(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        self.shared = nn.Sequential(
            nn.Linear(obs_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
        )
        self.actor = nn.Linear(128, act_dim)
        self.critic = nn.Linear(128, 1)

    def forward(self, x):
        h = self.shared(x)
        logits = self.actor(h)
        value = self.critic(h)
        return logits, value


def compute_gae(rewards, values, dones, last_value, gamma, lam):
    advantages = np.zeros_like(rewards)
    last_gae = 0.0
    for t in reversed(range(len(rewards))):
        next_value = last_value if t == len(rewards) - 1 else values[t + 1]
        next_non_terminal = 1.0 - dones[t]
        delta = rewards[t] + gamma * next_value * next_non_terminal - values[t]
        last_gae = delta + gamma * lam * next_non_terminal * last_gae
        advantages[t] = last_gae
    returns = advantages + values
    return advantages, returns


def train_parallel():
    envs = gym.vector.AsyncVectorEnv([
        lambda: gym.make("FlappyBird-v0", use_lidar=False) for _ in range(NUM_ENVS)
    ])

    obs_dim = envs.single_observation_space.shape[0]
    act_dim = envs.single_action_space.n

    model = ActorCritic(obs_dim, act_dim).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)

    obs, _ = envs.reset()
    best_score = 0

    pbar = tqdm(total=TOTAL_UPDATES, desc="PPO å¹¶è¡Œè®­ç»ƒ")

    for update in range(TOTAL_UPDATES):
        # å­˜å‚¨ rollout
        obs_buf = []
        act_buf = []
        logp_buf = []
        rew_buf = []
        done_buf = []
        val_buf = []
        score_buf = np.zeros(NUM_ENVS, dtype=np.int32)

        for _ in range(STEPS_PER_UPDATE):
            obs_t = torch.FloatTensor(obs).to(DEVICE)
            with torch.no_grad():
                logits, values = model(obs_t)
                dist = Categorical(logits=logits)
                actions = dist.sample()
                logp = dist.log_prob(actions)

            next_obs, rewards, terms, truncs, infos = envs.step(actions.cpu().numpy())
            dones = np.logical_or(terms, truncs).astype(np.float32)

            if "score" in infos:
                try:
                    score_buf = np.array(infos["score"], dtype=np.int32)
                except Exception:
                    pass

            obs_buf.append(obs)
            act_buf.append(actions.cpu().numpy())
            logp_buf.append(logp.cpu().numpy())
            rew_buf.append(rewards)
            done_buf.append(dones)
            val_buf.append(values.cpu().numpy().squeeze(-1))

            obs = next_obs

            # è®°å½•æœ€é«˜åˆ†
            if "final_info" in infos:
                for i in range(NUM_ENVS):
                    f_info = infos["final_info"][i]
                    if f_info and "score" in f_info:
                        if f_info["score"] > best_score:
                            best_score = f_info["score"]
                            torch.save(model.state_dict(), MODEL_PATH)

        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        with torch.no_grad():
            obs_t = torch.FloatTensor(obs).to(DEVICE)
            _, last_values = model(obs_t)
        last_values = last_values.cpu().numpy().squeeze(-1)

        rewards = np.array(rew_buf)
        values = np.array(val_buf)
        dones = np.array(done_buf)

        advantages, returns = compute_gae(rewards, values, dones, last_values, GAMMA, LAMBDA)

        # å±•å¹³
        obs_arr = np.array(obs_buf).reshape(-1, obs_dim)
        act_arr = np.array(act_buf).reshape(-1)
        logp_arr = np.array(logp_buf).reshape(-1)
        adv_arr = advantages.reshape(-1)
        ret_arr = returns.reshape(-1)

        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        adv_arr = (adv_arr - adv_arr.mean()) / (adv_arr.std() + 1e-8)

        # PPO æ›´æ–°
        batch_size = obs_arr.shape[0]
        idxs = np.arange(batch_size)

        for _ in range(EPOCHS):
            np.random.shuffle(idxs)
            for start in range(0, batch_size, batch_size // MINIBATCHES):
                end = start + batch_size // MINIBATCHES
                mb_idx = idxs[start:end]

                mb_obs = torch.FloatTensor(obs_arr[mb_idx]).to(DEVICE)
                mb_act = torch.LongTensor(act_arr[mb_idx]).to(DEVICE)
                mb_logp_old = torch.FloatTensor(logp_arr[mb_idx]).to(DEVICE)
                mb_adv = torch.FloatTensor(adv_arr[mb_idx]).to(DEVICE)
                mb_ret = torch.FloatTensor(ret_arr[mb_idx]).to(DEVICE)

                logits, values = model(mb_obs)
                dist = Categorical(logits=logits)
                logp = dist.log_prob(mb_act)
                entropy = dist.entropy().mean()

                ratio = torch.exp(logp - mb_logp_old)
                surr1 = ratio * mb_adv
                surr2 = torch.clamp(ratio, 1.0 - CLIP_RANGE, 1.0 + CLIP_RANGE) * mb_adv
                policy_loss = -torch.min(surr1, surr2).mean()
                value_loss = (mb_ret - values.squeeze(-1)).pow(2).mean()

                loss = policy_loss + VALUE_COEF * value_loss - ENTROPY_COEF * entropy

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
                optimizer.step()

        pbar.set_postfix({"æœ€é«˜åˆ†": best_score})
        pbar.update(1)

    pbar.close()
    envs.close()
    print(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° {MODEL_PATH}")


if __name__ == "__main__":
    train_parallel()

```